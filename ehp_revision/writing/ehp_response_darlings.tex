# R1 Comment 1

- Ecological fallacy vs individual fallacy, especially in the disaster context
- Spatial misalignment / ecological effects of aggregating data at the county
(or other geopolitical) level. 
- Could be particularly problematic in some high-risk areas, because of the 
"long and narrow" characteristics of island counties. Dare County, Key West. 

We think this is a very interesting question for disaster epidemiology, as
while there remains potential for ecological bias, the potential pathways for
health effects may also pass through damage in the individual's community beyond
their residence. For example, damage to major roads in the community might make
it harder for many residents to reach medical care in the aftermath of the
storm, even if their home did not experience extensive destruction. Similarly,
since power is provided over a network, conditions in an individual's community
can be important in determining risk of long-term outages (does Seth have
something on this?). As another example, if hospitals in the community are 
over capacity or have to evacuate, this could increase health risk for people in 
a fairly large "catchment" area for that hospital. 

Therefore, while we agree that individual-level data would in many cases be
helpful for tropical cyclone epidemiology, the ideal might be to design studies
that integrate both community-level and individual-level metrics of exposure.
This could help in determining if there are risks introduced to individuals
based on storm-related hazards in their broader community, after controlling for
the hazards experienced directly at their homes.

There is already some discussion on this topic in the Discussion [exposure 
measurement error / misclassification].

There are two interrelated mechanisms at play regarding this question. The
first is the potential for ecological bias. This bias results from inferring
individual-level associations from data that is available at an aggregated level. 
In the case of epidemiological research, these data include health outcomes, 
exposure, and covariates if the model is adjusting for confounding from them.
For example, in tropical cyclone epidemiology, an ecological study might 
compare county-wide rates of COPD hospitalizations to county-wide measures
of tropical cyclone exposure while controlling for county-level smoking 
rates and age distribution. The model in this case is fit at the county level, 
and so the relevant inference is at that level. If this county-level estimate
is inferred to represent the individual-level association between tropical 
cyclone exposure and risk of COPD hospitalization, controlled for smoking and
age, then the possibility of ecological bias arises. Given that this bias
results from inferring from a model with data at one level of aggregation
to another, this bias is also called cross-level bias. 

There are some subtleties that arise regarding this question for disaster
epidemiology. First, it has been argued that, for some ambient exposures
like weather and air pollution, contextual effects (that is, at the community
level) are helpful to estimate in their own right, not just as a potential 
surrogate for individual-level associations. This is because regulations
(in the case of pollutants), interventions, and planning often happen at
this level. 

Second, for a disaster, the relevant exposure might be not just at the
individual level (e.g., winds or flooding at the individual's residence), but
also throughout a broader area surrounding the individual. If storm damage
closes roads in a county, for example, or damages the power grid, then these
could create pathways for health risks for individuals throughout the county,
whether damage was high at their residence or not. The causal pathways for
tropical cyclones to affect human health differ from those for a dangerous
substance, like air pollutants, in which the substance itself must enter the
body to cause harm. While some health risk comes directly from the storm (e.g.,
deaths and injuries from trees falling on homes or drowning from flooding),
there are many more pathways that are indirect. These include pathways that go
through the way that the storm's damage affects community infrastructure and
access to medical care. Several papers have discussed the potential for 
"individualistic fallacy", as a counterpoint to "ecological fallacy", when 
such contextual-level exposure pathways are ignored. 

There are study designs that can be used to leverage ecological data while
minimizing risk from ecological bias. For example, potential confounders like
age distribution and smoking rates vary much less within a county over time than
comparing between counties. The mechanism for ecological bias depends on the
joint distribution between individual exposure, outcome, and covariates, if any
are included in the model. Time series-style study designs, which compare a
county to itself over time, allow for very similar covariate distributions
between exposure and non-exposure. Other studies add to this design by
stabilizing for temporal confounding through the addition of counties that were
never exposed, allowing for a differences-in-differences style approach to
calibrate for seasonal or longer-term trends that might otherwise create
confounding. For example, many health outcomes have a strong seasonal trend,
with peak rates in the winter and lows in the summer. Since the hurricane season
stretches from summer into fall, a study design that compares the rate of a
health outcome in an exposed county to the rate two weeks before the exposure
might be biased away from the null, since baseline rates of the health outcome
will typically be moving up over most of the hurricane season.

Also, there may be confounders that are relevant at the contextual, rather than
individual level, as well as modifiers. Example: whether the county is coastal
could be a contextual-level confounder and effect modifier. This will influence
whether the county is exposed to that storm or not, since storms usually weaken
rapidly when the center is over land. In terms of counfounding pathways, coastal
communities might tend to have lower levels of air pollution, because sea
breezes clear the pollution regularly. They might also be wealthier on average,
since property on or near the beach is desirable. Finally, they might be better
prepared for or more hardened against tropical cyclones at the community-wide
level (e.g., through hardier power infrastructure, more rigorous building codes,
higher likelihood of evacuating in advance of a threatening storm) compared to
nearby inland counties. 

Other approaches have also been suggested to reduce ecological bias while
leveraging ecological data, which might be available at a much larger
scale---and so provide more power---than is practical in collecting individual
data. These include multi-level approaches in which individual-level data from a
sample of individuals in each community is incorporated.

The second issue is exposure measurement error/misclassification. When the
exposure (e.g., binary exposure or continuous measure of intensity of exposure)
is only measured at the county level, all the individuals in that county are
assigned that same exposure. However, the hazards included in this study will
vary across each county. For some hazards, this variation will be more stark.
For example, tornadoes can be very local, causing extreme damage to a row of
homes and none to the homes across the street. Some types of floods can also be
very localized, rather than creating widespread flooding throughout the county.
In both cases, when a county-level exposure assessment is made based on an event
in the county in association with the storm, there may be many individuals in
the county who did not experience that hazard at their residence. The other
hazards we included---storm-associated winds and rainfall---will also have some
variation across counties, but this will often not be as stark. The rainfields
for tropical cyclones are very large, and while there are rainbands within the
storm that might have particularly high rates of precipitation, these progess
over the course of the storm, and it is unlikely that a county will have one
area that experienced very extreme precipitation while another experienced very
little. Winds might have more variation, especially in coastal counties and
counties near the storm's central track, since winds decay quickly as you move
from the storm's center, and storms typically weaken rapidly once they make
landfall and can no longer draw on warm ocean waters for energy.

This exposure data is provided at the county level, and this can lead to
exposure measurement error / exposure misclassification in studies that infer to
the individual level. In this case, there will be several processes at play.
First, the hazards of the storm will not be uniform across the county. The
degree of this within-county spatial heterogeneity will usually vary by hazard.
The within-county heterogeneity is likely lowest for rain and for wind at lower
levels. For rain, there can be particularly severe rain bands, but often a
storm's rainfield will bring high precipitation to a large area. Wind can have a
high heterogeneity close to the storm's center (storm winds decay most quickly
with distance near the storm's center) and near landfall (winds decay rapidly
after landfall). Other hazards can be very local within the county, only
affecting a small proportion of the total county area. A tornado is a clear
example---while it causes severe destruction in its track, areas just off the
track can be unharmed, creating a dramatic spatial gradient in damage within a
county. Floods can follow both. Some hurricanes have caused widespread flooding
(e.g., Floyd, Katrina) while some have caused flooding that is more spatially
focused (inland, with flashfloods? coast flooding just near the coast?). For
tidewater/coastal areas, there is often a dense network of waterway. When rains
occur further up the watershed, these all become flooded.

Second is the question of whether exposure measured at the resident's location
would be ideal. This is the case typically when studying an exposure that is a
chemical substance. In this case, the pathway depends on the substance entering
the body. For tropical cyclones, health risk could result from stress, problems
accessing medical care, or repercussions of power outages (higher exposure to
outdoor hazards like heat and air pollution, problems storing food and using
some medical equipment). For these, high levels of wind and other hazards in the
person's community may be as or more important than at their precise location.

Measurement error can be either random or systematic. Systematic error can often
be corrected with adjustment if the direction and typical size of the error is
understood. Random error cannot in the same way. Either type of measurement
error can be either differential (associated with the probability of the
outcome) or non-differential (independent of the distribution / probability of
the outcome). In simpler models, this characteristic might help in predicting
whether the resulting bias is likely toward the null; however, more complex
models (e.g., statistical models with adjustment for potential confounders) are
trickier to diagnose in terms of the likely implications of differential versus
non-differential measurement error [?].

Not only can the exposure be measured with error, but so too can potential
confounders and potential effect modifiers.

We provide exposure assessments at the county level. This spatial scale allows
for easy integration with health outcome data aggregated at the county level.
However, there are also some things to consider when modeling associations
at an ecological rather than individual level. The considerations are 
particularly nuanced when investigating the health risks associated with a 
disaster. 

Often in epidemiology, a study aims to estimate how an exposure changes the
risk of a health outcome for an individual. The data might be collected for
lots of people, but the key question is this---if a person is exposed, how 
does his or her risk of the outcome change. If you are exposed to this hazard, 
how much more likely are you to have the outcome? To get sick, or injured, or
to die? This inference is at the individual level. 

Even when individual-level inference is the goal, sometimes data is only 
available at an aggregated level. For example, privacy concerns might
mean that health outcomes are only made available as a county-wide daily 
count. You can fit a mode with this aggregated data, which will estimate the
association between exposure and health risk at the ecological level. This
contextual effect estimate is sometimes used as an estimate of individual-level
association, but doing so can introduce ecological/multi-level bias. The 
individual-level effect estimate can be very biased from the true association, 
even to the point of reversing the effect estimate---estimating a protective
effect when the true effect is detrimental, for example.

This ecological fallacy plays out in two ways. First, if you use a single 
exposure measurement for everyone in an area, some people will be misclassified
(or have exposure measured with error, for a continuous metric) unless the 
exposure is perfectly homogeneous across the area. This will typically not
be the case. This exposure misclassification can bias estimates of the 
association between exposure and outcome in the same way exposure 
misclassification through any other mechanism would.

Next, ecological bias could result from confounding, even if the confounders
are controlled in the ecological-level model. When you aggregate data, you 
lose information about how both the exposure and potential confounders vary
across individuals within the area of aggregation. As a result, a factor
could still confound the inference of an individual-level association, even 
if it is controlled for at a population level in an ecological model. For 
example, a study of the association between risk of pre-term birth and 
tropical cyclone exposure could control for county-level smoking when 
modeling county-level storm exposure and county-level rates of pre-term births.
Even with this control, an observed association could result from differences
in individual smoking status, if there is within-county variation in 
smoking and if this has a different pattern across people in the county than
variation in exposure from the county-wide exposure estimate.

For disasters, there are added nuances. First, for exposure it may not just 
matter what the level of exposure is in a person's immediate scope. Disasters
bring physical hazards that can harm people directly, but also through indirect
pathways. For example, a tropical cyclone can bring high winds that cause 
power outages, and as a result those affected could be exposed to more outdoor
hazards (outdoor air pollution, heat), struggle to safely store perishable
food and medications, and lose means to power medical equipment. While extreme
winds at a person's residence would increase their risk of a power outage, 
outages could also be caused by damage to the grid in another part of the 
community. In some cases, then, the level of exposure in a person's community
may be as important in opening a pathway of risk as exposure at the person's
immediate location. 

What's more, if the disaster has a large health impact, the health outcome of
one person in the community could affect the risk of the outcome (or other
adverse outcomes) for others. This situation is often only the case for 
infectious diseases, where one person with the disease can spread it to others.
However, if the community-wide impact is large enough, it can affect access
to and effectiveness of medical care for everyone in the community. This
effect has been seen recently with Covid 19---attempts to "flatten the curve"
aim to avoid moving into a state where a community's health system becomes
overwhelmed and can no longer deliver a typical level of care to those in the
community. This effect could happen with either infectious or non-infectious
diseases.

When a proxy exposure estimates is used for many people covered by a study, this
can result in a type of exposure measurement error called *Berkson error*. In
this case, the true exposure of each individual is randomly distributed around
this proxy/aggregate-level exposure estimate, with a mean exposure level among
the subjects that is equal to the proxy/aggregate-level measure. In other words,
the group as a whole is assigned a common exposure level, when in fact this is
the average exposure level across members of that group, but the individuals'
true exposure levels are randomly distributed around this mean group level to
which they are all assigned for modeling.

Berkson: 

$$
T = X + E
$$
where: 

- $T$: true individual exposure
- $X$: proxy exposure, assigned to everyone in the group
- $E$: error, with mean of 0, independent of $X$

The dataset provides exposure by group/area (county), not by individual. These
ecological exposure estimates require us to assume that the exposure level is
the same for all individuals in that county. It ignores/smooths over
within-county variability in exposure.

For wind and rain, we're essentially basing exposure assessment on modeling of
exposure. For rain, this is a based on a re-analysis model that
incorporates/integrates observed data from the area and time, using a model to
integrate data from diverse sources. For wind, the exposure estimates are based
on a model, with a core based on the Willoughby wind model. For flooding and
tornadoes, the exposure is based on records of events anywhere in the county,
and in many cases may be based, essentially, on exposure assessment, as in many
cases someone like someone at a NOAA [?] center have determined whether the
event should be reported in the database.

Even exposure at each person's residence would not suffice, since people move 
around during the day (although perhaps less for most people during a severe
storm).

Data that are aggregated across a population---for example, the total number of
deaths in a geographic area in a certain time period---is known as *ecological
data*, and studies that base inference on this type of data are known as
*ecological studies*. Terms include *ecological*, *aggregate*, *contextual
level*.

> "In an ecological study the unit of analysis is a group of people rather than 
the individual." [@sedgwick2014ecological]

> "In his seminal paper, Robinson (1950) distinguished between two types of
correlation---ecological and individual. The former is obtained for a group of
people, while the latter is estimated for indivisible units, such as
individuals." [@portnov2007ecological]

Here is one challenge when working with aggregated data. If you use aggregated
data to infer individual-level associations, this can lead to *ecological fallacy*. 
This results from cross-level inference---the data used to model the association
is at the contextual level (e.g., county-level), while the inference is for the
individual association between exposure and outcome. 

One definition of the ecological fallacy: 

> "This spurious result provides an example of the ecological fallacy, in 
which conclusions ... based on ecologic data are opposite of those drawn 
on the basis of individual-level data." [@wakefield2008overcoming]

> "It is a bias produced when analyses realised in an ecological (group level)
analysis are used to make inferences at the individual level."
[@delgado2004bias]

> "Results from ecological studies are prone to the ecological fallacy. The 
ecological fallacy is a term used when collected data are analysed at a 
group level and the results are assumed to apply to associations at the 
individual level." [@sedgwick2014ecological]

Importantly, aggregated data can be used both to infer a contextual effect
(e.g., the association between county-wide rates of a health outcome and 
exposure) as well as an individual-level effect (e.g., how a person's
risk of the outcome is associated with that person's exposure to the hazard). 
The first inference will *not* be prone to ecological bias, as the inference
is at the same level as the data. The second inference (individual-level) 
will be susceptible to bias when aggregated data is used to fit the model. 
In this case, the association estimated based on aggregated data can be 
different, and even reversed, compared to the true individual-level effect. 

> "I propose three criteria for the identification of ecological fallacy; all
three of these should be present to confirm its existence: (1) Results must be
obtrained with ecological (population) data; (2) Data must be inferred to
individuals. One use of ecological studies is to explore individual-level
association when individual data are not available. When the focus of the study
was contextual or based on population effects and there is no inference to
individuals, ecological fallacy is not possible. When only the first two
criteria are present---which is insufficient to affirm ecological fallacy---it
is appropriate to acknowledge that there is a possible relationship and that
further study is required; (3) Results obtained with individual data are
contradictory." [@idrovo2011three]

> "One important breakdown in the analogy between ecologic-bias and
individual-level confounding occurs because in etiologic research, the target of
inference for both ecologic and individual-level studies is the same: Both study
effects at the individual level. For an individual-level study, these target
effects are at the same level as the units of analysis. But, for an ecologic
study, these target effects are at a finer level than the units of analysis. As
a result, an ecologic study can be unbiased for ecologic effects (in particular,
ecologic confounding may be absent) and yet still be biased for individual-level
effects. Many of the classic social science examples of ecologic bias are of
this form." [@greenland1994invited]

> "We refer to an incorrect extrapolation from unbiased estimates of ecologic
effects to unobserved individual-level effects as 'cross-level bias' (although
the latter term is sometimes used to refer to any ecologic bias). As the
examples suggest, it is possible to detect cross-level bias if one can identify
and observe homogeneously exposed regions." [@greenland1994invited]

> "assuming that associations observed at the level of the area hold for the 
individuals within the areas can lead to the so-called ecological fallacy 
(Selvin, 1958)." [@wakefield2006health]

> "A broad definition of ecological fallacy is that certain data (e.g.,
illiteracy rates by foreign-born) are unavailable, leading to the use of proxies
and erroneous estimates. However, we advocate a more narrow definition of this
phenomenon, according to which 'ecological fallacy' refers solely to differences
in conclusions which may be drawn from group-level data (e.g., average
illiteracy rates by foreign-born), as opposed to data obtained for individuals."
[@portnov2007ecological]

> "If different conclusions are drawn from the analysis of data upon their
aggregations into units of different sizes (e.g., from individuals to townships
and regions), these differences are commonly referred to as 'ecological
fallacy'. In epidemiology, these differences are also known as ecological or
cross-level bias [refs]. The ecological bias is difficult to control and may
lead, under certain circumstances, to spurious effects [refs]. If
epidemiological study does not consider the possibility of ecological bias,
misguided policy decisions may follow [refs]." [@portnov2007ecological]

Ecological bias can *only* happen if there is, indeed, an individual-level
effect:

> "If exposure has no effect on any individual, there will be
no individual-level or ecologic effects, and so cross-level bias cannot occur. 
Thus, cross-level bias will not affect the validity of an ecologic test of the 
null hypothesis, although it still must be considered in interpreting a 
significant effect." [@greenland1994invited]

In some cases, the ecologic-level effect (contextual effect) will be directly 
of interest, and so we may be interested in inferring at that level (in which 
case use of ecologic-level data is appropriate and will not be prone to 
ecological bias):

> "The bias phenomenon in example 7 may be ascribed to the fact that an 
ecologic (regional) variable (percent Protestant) had effects on individual 
risk, in addition to effects of the corresponding individual-level variable
(religion) that the ecological variable summarized. In both social and 
infectious disease epidemiology, as well as in community intervention studies, 
ecologic effects may be of direct interest; a classic example is the 
phenomenon of 'herd immunity', in which the overall prevalence of 
immunity in a region, as well as individual immune status, determines the 
risk of individuals within the region (16). For noncontagious diseases, 
however, ecologic effects may not be of direct interest, and may in fact
obscure the individual-level effect of interest." [@greenland1994invited]

In some cases, the estimated association based on ecological data will be very 
different from the association that exists at the individual level: 

> "We see that the ecological relative risk associated with low birth 
weight is completely incomparable with the individual-level coefficient."
[@wakefield2008overcoming]

Ecological bias can also complicate estimation of effect modification, which 
otherwise could help in identifying vulnerabilities and susceptibilities among
certain subpopulations: 

> "Furthermore, non-White race now appears protective, rather than detrimental
as the individual-level analysis suggests." [@wakefield2008overcoming]

Ecologic bias crops up because the aggregated data fails to capture variation
within each aggregation level (county, for example) for both the exposure 
and potential confounders. For example, during a tropical cyclone, different
parts of a county will experience different peak wind speeds. If the county 
is coastal, the most severe winds will likely be near the coast, as the 
storm will weaken once it makes landfall and moves inland, as the storm 
generates its power from the water. When we assign the entire county a
single estimate of intensity, it smooths over the fact that locations in 
some parts of the county were subject to higher peak winds while locations
in other parts of the county had lower winds. At the least, this smoothing
of variation in exposure levels across the county could lower the power of 
the study to detect a clear association between exposure and health risk, as
this smoothing drops information inherent in the variation within the county
in exposure levels. 

This, however, would not result in ecological fallacy unless another factor
comes into play---variation within each aggregation level in confounders. 
Just as aggregation smooths over within-area variation in exposure levels, it
also smooths over within-area variation in levels of potential confounders. 
Depending on the patterns of this within-area variation, a result could be
that ecological-level control of the confounders does not, in fact, control
for their role at the individual level, and so the association inferred at
the ecologic level continues to be confounded by them when inferred to the
individual level. 

Essentially, this comes down to a question of the joint distribution, at the
individual level, of exposure, outcome, and covariates [?].

> "As various discussants have pointed out (3--6, 8, 9), ecologic relative-risk
estimates can be subject to biases not present in estimates from individual-level
observational studies of the same populations (case-control and cohort studies).
Unlike an individual-level study, an ecologic study does not link individual
outcome events to individual exposure or covariate histories, nor does it
link individual exposure and covariate histories to one another. It is these
linkage failures that are the source of the special biases of ecologic studies
(1, 2, 6)." [@greenland1994invited]

> "In the extreme, ecologic bias in comparisons limited to homogeneously exposed
regions (as in examples 3--6) can be viewed as purely an issue of confounding,
albeit with special complexities of measurement and control of confounders."
[@greenland1994invited]

> "More generally, one may show that cross-level bias will not occur if the 
individual-level effects of all variables (including unmeasured background
factors) follow a multiple linear-regression model with no regional effects and
no interactions (17). Nevertheless, given the usual inappropriateness of the 
multiple-linear model and the absence of homogeneously exposed regions, the 
possibility of cross-level bias adds a dimension to ecologic bias beyond
that of simple confounding." [@greenland1994invited]

> "Covariate control in ecologic studies requires attention to details not 
ordinarily of concern in individual-level studies. When, as is usually the
case, important nonlinearity or nonadditivity can be expected among 
exposure and covariate effects, it may be necessary to obtain and 
control for multiple summaries of joint covariate distributions in order
to insure that control is adequate." [@greenland1994invited]

> "Ecological fallacy can be produced by within group (individual level)
biases, such as confounding, selection bias, or misclassification, and
by confounding by group or effect modification by group." [@delgado2004bias]

> "**Confounding by group**: it is produced in an ecological study, when 
the exposure prevalence of each community (group) is correlated with 
the disease risk in non-exposed of the same community. It can be 
a mechanism for producing ecological fallacy." [@delgado2004bias]

If individual-level inference is the aim, and population-level data is
available, there are some methods for using it while still aiming to avoid
ecological bias. Indeed, it can be helpful to use population-level data, as it
is often available for a large population, improving the power and precision of
the study. Further, the level of exposure might vary a lot more over the
population captured with population-level data compared to the variation that
captured in a smaller sample of individual-level data. This can contribute both 
to statistical power and improve external validity (as the study data will cover
more of the range of exposure that might ever be expected). However, if
individual-level inference is the goal, then there are ways to supplement
population-level data with samples of individual-level data through two-level, 
semi-ecologic study designs.

> "It is generally well recognized that in order for ecologic data to provide
reliable inferences, they need to be supplemented with individual-level data."
[@wakefield2008overcoming]

> "For this example, we have access to complete individual-level data,
permitting a 'gold standard' individual-level analysis."
[@wakefield2008overcoming]

> "The only reliable way to characterize within-area variation in exposures
and confounders, and hence control ecologic bias, is to collect and incorporate
individual-level data. To help epidemiologists achieve this goal, in this paper
we describe the use of the two-phase design in an ecologic setting."
[@wakefield2008overcoming]

> "The only reliable way to characterize within-area variation in exposures
and confounders, and hence control ecologic bias, is to collect and incorporate
individual-level data. To help epidemiologists achieve this goal, in this 
paper we describe the use of the two-phase design in an ecologic setting."
[@wakefield2008overcoming]

> "In a semi-ecologic study, an ecologic exposure is combined with
individual-level outcomes and confounders. A two-phase approach is particularly
useful for such a study, with the phase 2 data corresponding to stratified
sampling of individual exposures." [@wakefield2008overcoming]

> "To implement a two-phase design, an initial phase 1 cross-classification by
the binary disease outcome and stratification variables is required; in phase 2,
samples of individuals are drawn from each of the cross-classification cells,
with data on additional variables being drawn from the subsamples of
individuals. Intuitively, the stratified sampling is focused on informative
cells, and estimation methods use both phases of data for efficiency and to
acknowledge the outcome-dependent sampling. In the simplest ecologic setting,
the cross-classification is by outcome and area only, and if area is a surrogate
for important risk factors this design will be efficient. We are particularly
interested in situations where an initial classification is available by
outcome, area, and confounders such as age and gender---this is the case in a
semi-ecologic study. Phase 2 may then provide detailed exposure information on a
subset of the phase 1 individuals." [@wakefield2008overcoming]

> "Two-phase study designs are a generalization of matched case-control designs
in which, initially, the entire sample population is cross-classified according
to case/control status and some stratification variable, S. The latter depends
on covariates observed in all individuals and may include exposures of interest, 
proxy exposure measures, or potential confounders. In settings like those we
are considering, such as environmental epidemiology, S may also depend on geographic
area, which can act as a surrogate for the totality of confounders associated
with each area, as well as provide a well-defined sampling frame for the controls."
[@wakefield2008overcoming]

> "The results of our simulation study point to the benefits associated with
combining the two sources of data, in terms of both bias and efficiency. Rather
than supplement an ecologic study with individual-level data, it may be of
interest to combine existing individual-level data with external group-level
data. Strategies that combine both types of data have been shown to alleviate
participation bias and improve efficiency in case-control studies with missing
data (31)." [@wakefield2008overcoming]

> "As with other observational studies, ecologic studies can give useful results
if biases such as those discussed here can be ruled out or quantified.
Nevertheless, bias evaluation can be especially difficult in ecologic studies of
geographic regions because of the many potentially interacting covariates that
may differ across regions. When biases cannot be ruled out with available data,
further exploration will require individual-level studies."
[@greenland1994invited]

Why we might sometimes use (and want to use) population-level data: 

> "Because of the unavailability of individual-level data, ecological data may
be resorted to and may come in a variety of forms." [@wakefield2008overcoming]

> "Epidemiologists continue to use ecologic and aggregate data. Despite their 
known drawbacks, these data, often aggregated across geographic areas, offer the
advantages of widespread availability and gains in statistical power from large
populations and increased exposure ranges." [@wakefield2008overcoming] 

> "Population and health data are often routinely available in ecological, that
is group, form while the exposure data typically consist of a set of values
recorded at monitor sites, or via one-off sampling." [@wakefield2006health]

It can be important to think about the scale that the process happens at. For
something very local, you might lose a lot more with aggregated data. 

> "Data availability and exposure variability often determine the scale of
examination and the suitability of a study. Exposures arising from a point or
line source offer exposure contrasts on small scales, requiring small-area data;
in constrast, dietary variables show little variation across small scales, and
consequently international studies are used." [@wakefield2008overcoming]

There can also be an *individualistic fallacy*, if emphasis is limited to the
association between individual-level factors, without considering the potential
role of contextual factors in an individual's risk of a health outcome. In other
words, cross-level bias can go in both directions [@idrovo2011three]. While the
fallacy of inferring individual-level associations from ecologic-level data is
called *ecological fallacy*, the fallacy of inferring ecologic-level
(contextual) effects from individual-level data is known as the *atomistic
fallacy* [@idrovo2011three]. There are also fallacies that can come in, it
seems, from inferring at the same level as the data but failing to consider
influences from factors at the other level. When inferring individual-level
associations from individual-level data, without considering an additional role
of ecological-level factors, this is known as the *psychologistic* or
*individualistic fallacy* [@idrovo2011three]. When inferring contextual effects
from ecologic data without considering individual-level factors, this is known
as the *sociologistic fallacy* [@idrovo2011three].

> "Although ecological studies are important to epidemiology (especially in
environmental and social epidemiology), public health practitioners seem afraid
of ecological studies. It is a common practice to assume the presence of
ecological fallacy (Robinson 1950) and low-level validity when analyzing an
ecological study. Most epidemiologists prefer an exclusive individualistic
approach, although the importance of a multilevel causal approach is widely
recognized (Diez-Roux 2002). In this sense, some authors suggest that it is as
important to recognize the presence of ecological fallacy as to recognize
psychologistic or individualistic fallacy (Subramanian et al. 2009)."
[@idrovo2011three]

Confounding in general: 

> "A confounder ... is associated with an exposure or risk factor for the
outcome and with the outcome independent of the exposure, but is not on the
causal pathway between the exposure and the outcome. Confounding occurs when the
relationship between an exposure variable and the outcome variable is
contaminated, so that the measure of association between these two variables is
actually also capturing the effects of a third variable, the confounding
variable. For example, many factors are associated with income (exposure) and
health (outcome), such as education, employment or wealth? It may be that these
factors explain part of the observed assocation." [@gunasekara2008glossary]

Measurement error in general:

> "Measurement error: The different between an observed variable and the
variable that belongs in a multiple regression equation. Measurement error is
the random or systematic error arising during data collection of variables. The
measured variable x is measured with error eta, which is the distance from x to
the 'true' value of x. Measurement error can produce bias such as attenuating
the estimators of exposure variables, but may also have more complex effects.
Longitudinal data analyses such as fixed effects models can actually augment the
problem of measurement error in mis-measured explanatory variables that change
little over time." [@gunasekara2008glossary]

Study validity: 

> "Study validity refers to the degree to which the inferences drawn from a
study are warranted when account is taken of the study methods; the
representativeness of the study sample; and the nature of the population from
which it is drawn.[5] There are two types of study validity. *Internal validity*
is the degree to which the results of a study are correct for the sample of
people being studied. *External validity* (generalisability) is the degree to
which the study results hold true for a population beyond the subjects in the
study or other settings.[20]"

# R1 Comment 2


- Differs by exposure metric. Easier for some (wind, rain) than for others
(flood, tornado)
- For rain, we use a re-analysis product only available for the continental US
(right?). However, there are similar gridded re-analysis products available for
other regions worldwide that could be used in a similar way.
- For tornadoes, there may be national databases in other spots similar to the
tornado database that we use, but as far as we're aware, no international one.
- Same for floods. 
- For wind, the wind model could be generalizable with some tweaks. These
include:
  + More landmasks
  + Direction of storm movement for Northern vs Southern Hemispheres
  + Different wind averaging periods for the recorded best tracks data in
  different storm basins.
  + Our wind model was derived with US data and so may need bias-correction or a
  shift to a West Pacific--based model

> "Tornadoes long have been recognized as a global phenomenon (e.g., Wegener
1917; Feuerstein et al. 2005), having been recorded in every continent except
Antarctica. As such, numerous nationas share an interest in improving assessment
of their damage. International research and involvement in tornado survey work
and damage and intensity scales is well underway." [@edwards2013tornado]

For flood and tornado events, we used a database of severe weather events that
is specific to the United States (the US NOAA's Storm Events database). To
expand our exposure data for these hazards internationally, we would need to
find similar databases for other affected countries. [international flood and
tornado databases? EM-DAT? Others?]

The wind exposure data is based on modeling wind fields based on records of the
storm's track and central intensity throughout the tracking period. These values
are tracked for tropical cyclones around the world, and so it would be possible
to expand the dataset to encompass other countries for wind exposure
measurements. However, there are some practical issues that would need to be
addressed to do so with the approach we've taken.

The software we are using to model wind [@stormwindmodel] has several features
that make it specific to Atlantic basin storms. First, some steps in the model
depend on whether the storm's center or the location being modeled is over land
or water. These steps use a landmask---a grid with a binary indicator of land or
sea by latitude and longitude. Currently, the software only provides a landmask
that covers the United States, the Gulf of Mexico, and areas of the Atlantic
Ocean near the US. Second, the model assumes that cyclonic winds are
counterclockwise, and so is valid only for the Northern Hemisphere. Third, the
model assumes that the central intensity of the storm is based on [wind
averaging period], which is the standard used in the tracking data for
Atlantic-basin storms, but is not a common standard around the world. Instead,
some basins record central intensity measurements using averaging periods of
....

Therefore, it would be relatively straightforward to expand the wind exposure
data to cover other countries affected by Atlantic-basin storms, although this
would require some expansion of the area covered by our current landmask. More
modifications would need to be made to the wind model software before it could
be extended to storms in other basins, particularly basins that use different
wind averaging periods for their main tracking data and basins in the Southern
Hemisphere. Alternatively, other wind modeling software could be used for this
exposure assessment, as other groups have developed software that is more
extensible to multiple storm basins [Australian group's software].

The rain exposure data would also be fairly easy to extend to cover other
countries, although again there would be a few practical limitations to extend
the approach we've taken in developing the dataset covered in this manuscript.
For this exposure, we've used a re-analysis dataset. We've taken some steps to
translate this gridded data to the county scale, so it can be integrated with
the scale for many human impacts measurements. However, these steps have been
well documents [ref] and would be easy to reproduce. The re-analysis dataset
that we use is specific to the contiguous US [ref on NLDAS], but other similar
re-analysis datasets exist for other locations [example of one with global
coverage?].

For all of these, we have provided exposure assessments at the county scale. Any
of the steps we have taken could be done for other countries at a similar
geopolitical scale, although the name of that areal unit will vary by country
(e.g., municipalities in Mexico, districts in India). For wind and rain
exposures, the approach we take here could be used with any areal unit, as long
as there are geographical shape files available for the area boundaries
(although the modeling for wind may be to the area's geographic mean center
rather than population mean center).

> "We initially developed software to model exclusively Atlantic-basin storms
[@stormwindmodel]. There are two characteristics of Northwestern Pacific Ocean
basin storm data that prevent direct use of the current version of the software.
First, the software currently requires Western Hemisphere longitude inputs.
Second, the ``Best Tracks'' data from the China Meteorological Administration
include central tropical cyclone intensity using a 2-minute averaging period,
while the software currently assumes that central intensities are given for a
1-minute averaging period, as is the case for storm track data from the US
National Hurricane Center. With the assistance of Ms. Schumacher, we will update
our tropical cyclone wind model software to appropriately handle 'Best Tracks'
data from the China Meteorological Administration, incorporating conversion
factors [@harper2010guidelines] to convert central storm maximum sustained wind
measurements to a common averaging period prior to applying further steps of the
wind model."

# R1 Comment 3

In this paper, we present a dataset that focuses on exposure to the primary
physical hazards of wind, rain, tornadoes, and flooding. We do not include data
related to intermediate pathways, but such data could be a very interesting
future expansion of the data.

There are two elements that are important related to this question. First, many
of the health impacts of tropical cyclones will likely come through indirect
pathways, including through damage to property and infrastructure. Ultimately,
it would be useful to have large-scale databases of these secondary exposure
pathways to use for more complex studies of the pathways between the physical
hazards of these storms and how they affect human health. In this sense,
something like the FEMA declarations could be considered not as a proxy for
assessing exposure to physical hazards, but rather as a proxy of damage caused
by the storm, and so a measurement informative for one of these indirect
pathways from physical hazards of the storm to health impacts, through something
like a mediation analysis. We have added text to the manuscript on this point:


In terms of using FEMA declarations as a source for this, however, there are 
some limitations. These declarations are on average tied to the amount of 
damage and other impacts experienced following a storm, but there are a number
of other considerations that drive whether a disaster is declared or not. 
These contributing factors could vary by location (especially from one state
to another) and also over time, especially over the long period we've included
for our other data. Given that, these may not be the best data source to use
to try to capture estimates of the damage or impacts to a community in a multi-year, 
multi-site dataset like the one we present.

FEMA's disaster declarations are issued to provide assistance, based on damage
assessment, to individuals or the entire public in certain locations where
catastrophes overwhelm the state or local government [@mccarthy2014fema], and
any use of them for exposure assessment is secondary. Due to the political
nature of disaster declaration process, these declarations are often subject to
many political and economic factors [@mccarthy2014fema; @logue1981research]. One
epidemiological study specifically focused on exposure assessment for tropical
cyclone epidemiology and compared use of FEMA declarations with other methods of
exposure assessment, including one based directly on the storm hazard of wind
[@grabich2015measuring]. They found that exposure assessment based on FEMA
declarations tended to overassign counties to be "exposed," resulting in false
positives [@grabich2015measuring]. A further concern is that, given the
political and economic nature of these declarations, there is likely variation
across time and geography in the likelihood of a given exposure resulting in a
disaster declaration. This is particularly worrisome for a study that seeks to
explore risk across multiple years and affected communities.

Other sources of data may be better suited to use as a marker of damage to
assess that pathway of indirect health impacts from a tropical cyclone. For
example, data on storm impacts, including property damage or insurance claims
data, are sometimes available through storm databases or publications (e.g.,
NOAA's Storm Events data, *Monthly Weather Review*). While such data could
potentially be used to create an exposure metric, tropical storm damage data
requires some normalization, to incorporate both changes in dollar value over
time and also changes in development in at-risk areas, to be comparable over
extended time periods [@pielke1998].


However, evidence from prior tropical cyclone epidemiology that doing so is
problematic and should be avoided [@grabich2015measuring]. 

We have added some discussion of this point... 

- Future research could expand the work by [@grabich2015measuring] to determine
the extent of these issues with use of FEMA disaster declarations for
epidemiological exposure assessment. This would need to go beyond the comparison
with exposure assessment based on storm hazards, because sometimes the
characteristics of the affected community modifies the amount of disaster that
results from hazards of a certain severity. For example, a community that it
exposed often might be more "hardened" against damage from the storm (any work
from Seth on this) and so might experience less infrastructure damage from a
tropical cyclone with certain characteristics compared to a community that is
rarely exposed to tropical cyclones. One interesting direction would be to
investigate if there is evidence of differences in probablity of disaster
declarations at geopolitical boundaries, like when comparing counties on either
side of a state border.

There are some cases where damage reports are directly used to infer the
intensity of the weather event. For example, damage surveying is used following
a tornado to assign the tornado's class on the [enhanced] Fujita scale. However,
this is not the intent of FEMA damage reports, which instead are used to
determine post-disaster aid [?] and are influenced by a number of political
factors.

> "Because of the historic lack of direct measurements and remotely sensed
tornado wind speeds at or near group level, damage surveying has remained the
most common form for indicating tornado strength. ... The occurrence of a direct
tornado strict upon a fixed, sufficiently sturdy, and well-calibrated wind
measuring station is quite rare. Only 31 direct in situ tornado observations are
evident between 1894 and 2011." [@edwards2013tornado]

This question could be explored to some degree through a study that uses the 
physical hazard data provided in this dataset as the exposure and FEMA disaster
declarations as the outcome. Such a study could clarify the degree to which 
each hazard---and its intensity---helps in explaining that probablity that 
a disaster declaration is issued. It could also clarify the degree to which 
other factors, beyond the intensity of these hazards, drive the probability of
a county receiving a disaster declaration. For example, if two counties 
are exposed to a storm-associated hazard of the same intensity, they may 
still have different probabilities of associated societal impacts, as a 
community that is regularly exposed to that hazard may be more "hardened" 
against it. Other differences in infrastructure and factors associated with 
vulnerability could similarly introduce variability. 

At a more complex level, studies could explore the causal pathways from the 
initial physical hazard to resulting damage/property loss through to associations
with health outcomes, to provide a clearer idea of the direct and indirect 
pathways leading to health impacts from tropical cyclones.

For both of these studies, however, there may be better sources of damage
estimates to use than FEMA disaster declarations. FEMA disaster declarations 
can be prone to political and other influences, and since their primary function
is not to record damage but rather to be a step in getting help to communities, 
they can be problematic to use as an unbiased estimate of damage levels in 
epidmiological research. One study, for example, found ... . There are other
estimates of property damage that might be less problematic, including the 
estimates recorded in NOAA's Storm Events database and data from private 
insurers, although the quality of disaster loss data is well known to be
less than ideal for secondary research [can be prone to bias through a number
of mechanisms] [ref to Gall paper?].

FEMA declaration is largely subjective, requiring state governors and/or tribal
chiefs to submit a proposal for government aid on the basis that local resources
are overwhelmed [@FEMA]. As a result, using FEMA disaster declarations as
exposure classification to hurricanes restricts exposure to 1) those states
whose resources are overwhelmed and 2) those states who apply for FEMA aid.
(FEMA is likely less objective given societal effects (politics) [@kunkel1999])
As a result, other severely impacted counties and states that may not fit FEMA
criteria are excluded and may therefore be incorrectly identified as unexposed,
preventing researchers from assessing the full impact of a given hurricane
[JMF-- the Grabich 2016 study actually found that FEMA assigned the highest
exposure, and may overestimate the health effects of a hurricane; GBA-- let's
think about why that might have had "false positives". I think that would be an
interesting point to include here.]. Researchers have found heterogeneity in
exposure assignments of counties, both within and between storms, when different
metrics of exposure are used [@grabich2015measuring; @grabich2015]. Such
missclassification can have important consequences in assessments of the public
health and economic impacts of tropical storms.

One study used FEMA disaster declarations as a binary indicator of tropical
storm exposure [@grabich2015measuring]. They found that this metric tended to assess a
lot more counties as "exposed" to a storm than distance- or wind-based metrics
[@grabich2015measuring], although they only compared the two exposure metrics as applied
to four storms (Hurricanes Charley, Frances, Ivan, and Jeanne in Florida in
2004).

Data on storm impacts, including property damage or insurance claims data, are
sometimes available through storm databases or publications (e.g., NOAA's Storm
Events data, *Monthly Weather Review*). While such data could potentially be
used to create an exposure metric, tropical storm damage data requires some
normalization, to incorporate both changes in dollar value over time and also
changes in development in at-risk areas, to be comparable over extended time
periods [@pielke1998].


# R1 Comment 6

*What is DesignSafe:*

> "DesignSafe is the cyberinfrastructure platform that has been developed as
part of NHERI to support natural hazards engineering research, and
it succeeds the NEEShub cyberinfrastructure that was developed for the
earthquake engineering community through the Network for Earthquake
Engineering Simulation (NEES) program." [@rathje2017designsafe]

> "DesignSafe plays an important role in integrating the various NHERI
components and the research taking place at NHERI facilities, but also has the
broader goal of enabling transformative research in natural hazards and
engineering across the numerous technical disciplines engaged in this field."
[@rathje2017designsafe]

> "DesignSafe has been developed as a flexible, extensible, community-driven
cyberinfrastructure, and it embraces a cloud strategy for the big data generated
in natural hazards engineering. DesignSafe provides a comprehensive CI that
supports the full research lifecycle from planning to execution to analysis to
publication and curation." [@rathje2017designsafe]


> "The future of natural hazards engineering research requires the integration
of diverse data sets from a variety of sources including experiments,
computational simulation, and field reconnaissance, as well as a variety of
research disciplines including earth science, social science, building science,
and architecture. The DesignSafe cyberinfrastructure has been designed to
provide the functionalities that will enable transformative research in natural
hazards engineering. By adopting a cloud strategy, DesignSafe allows for a
fundamental change in the way that research is performed. It provides a
comprehensive cyberinfrastructure that supports research workflows, data
analysis, and visualization, as well as the full lifecycle of experimental,
field, and computational research required by engineers and scientists to
effectively address the threats posed to civil infrastructure by natural
hazards. The integration of data and computation in the cloud will enable new
research discoveries in natural hazards engineering, which in turn can lead to
more hazard-resilient civil infrastructure." [@rathje2017designsafe]

*Access to R:*

> "The real revolution takes place downstream of data generation. Here, data 
analytics and visualization are performed in the cloud within the *Discovery
Workspace* while accessing any data within the *Data Depot*. Researchers can 
invoke common analysis programs, such as MATLAB, as well as other analysis/
visualization tools, such as Jupyter notebooks. A Jupyter notebook is an 
electronic notebook that allows users to embed rich text elements as well
as computer code, graphs, and visualizations within a single notebook that
can be shared through the web. Over 40 different programming languages are 
supported in Jupyter, including Python and R, and MATLAB code can be easily 
converted, making Jupyter a versatile tool for research. Performing analysis in 
the cloud allows researchers to integrate and explore various data without
tedious downloads. Additionally, using a seamless cyberinfrastructure to 
complete all research tasks enables tracking and relating of the processes 
applied to data." [@rathje2017designsafe]

*Data Depot:*

> "the Data Depot, a flexible data repository with streamlined data management
tools." [@rathje2017designsafe]

> "As data progresses towards publication the requirements for metadata 
increase as metadata provides users with search and discovery functions. At
the end of the research project the user may edit the information for 
publication and complete the process of assigning digital object identifiers
(DOIs) and applying the appropriate license. On-demand assistance from a 
curator is available to provide training and guide users through their data
curation and publication needs." [@rathje2017designsafe]

# R3 Comment 2

"Synoptic meteorology" refers to ...
A key idea is that observations from a widespread geographical area can be
integrate to provide a picture of large-scale weather [features?], including
large areas of high or low pressure [?].

> "In meteorology, the term synoptic denotes simultaneous observations taken
globally, or at least over a large area." [@willoughby2007hurricane]

> "Nearly 20 years ago [from 1937] [Bjerknes] declared that numerous and 
important problems might be solved with the aid of observations already 
assembled in the archives of different bureaus, but that the difficulties
in making these observations serve a special purpose were unsurmountable
for the individual scholar. He added that, in particular, the synoptic 
charts representing the momentary states of the atmosphere over large
areas of the earth, in all the necessary details and exactitude, were 
excessively difficult to assemble. His proposal was that there be entered
at the central bureau of each country, on the appropriate maps, all the available
observations taken in that country and that there be employed, by the different
bureaus, maps that may be superimposed or juxtaposed." [@gregg1937international]

However, to integrate data from many sources to provide inputs for broadscale,
synoptic analysis, it is helpful to use some standards in collecting these data
at different locations. To facilitate numerical weather forecasting and other
synoptic meteorology, weather data are regularly collected at standardized
times, so that the data can be more easily integrated as an input to models.
This practice extends back over 100 years, when data were collected from many
[offices?] in the US by telegram [?] at regular times during the day and used to
create synoptic weather maps that captured major weather systems in the country.

> "In order to allow comparison of surface conditions at the same time
throughout the world, the groups regulating the collection of
hydrometeorological data have established a standard time for taking
observations. These observations will have times observed within the 10 minutes
preceding the UTC standard time. Hence surface observations for meteorological
use have timestamps reflecting 0000, 0300, 0600, 0900, 1200, 1500, 1800, 2100
UTC." [@usnwstime]

> "Since the end of World War II, observers have launched rawinsonde observation
worldwide at 00 and 12 UTC (coordinated universal time). High-threat situations,
such as impending TC landfall, may dictate observations at 6-h or even 3-h
intervals. Interpolating rawinsonde observations to the model's mesh and
starting the calculation at a synoptic time is the simplest way to prepare a
model initial condition." [@willoughby2007hurricane]

> "Because simple interpolation in space and time is a less-than-optimum way to
use observations, nonsynoptic data require elaborate four-dimensional
variational data assimilation schemes." [@willoughby2007hurricane]

> "By 1941, charts for the 750, 500, and 250 mbar surfaces were being prepared
daily for 0400, 1200, and 2000 GMT." [@grahame2000development]

> "During the period considered here, NHC issued advisories 3 h after the
standard synoptic time (0300, 0900, 1500, and 2100 UTC)." [@torn2012uncertainty]

> "The operation data assimilation and forecast cycle interval is 6 h"
[@aberson201010]

> "Dan Petersen asked whether a data deficiency really exists with all the
observational sources in the Atlantic region, or are the data not being properly
used in the numerical models? ... Steve Lord emphasized the difficulty of the
task because many of these observations are not obtained at the synoptic
times..." [@elsberry1992there]

> "In the modern era, forecasters rely on purely dynamical models that integrate
the Navier-Stokes equations for atmospheric motions to represent both the storm
itself and its surroundings. Numerical models are structured on a computational
grid with temperature, moisture, and wind tabulated in more-or-less rectangular
cells." [@willoughby2007hurricane]

> "Predicting the weather required a widespread but highly coordinated team of
observers and forecasters. Organization was key... They transmitted data from
the field using forms and protocols adapted from those that Abbe had developed
while forecasting the weather from Cincinnati, Ohio. The information arrived by
telegraph coded to maximize accuracy and minimize the word count and changes.
The reports piled up in a rush at the appointed hours." [@willis2006cleveland]

> "Three times daily the duty forecaster broadcast a synopsis of prevailing
weather conditions nationwide..." [@willis2006cleveland]

> "The requirement of synoptic data collection is easily met by the
'conventional' observing system now serving as the main source of meteorological 
data throughout the world, since it is based on human observations which could 
be scheduled at convenient synoptic times." [@morel1971initialization]

Since large-scale weather features are very larger, relevant data for a synoptic
map often comes from locations with different local times. The need for
synchronized weather data collection therefore actually played an important role
in establishing standardized time zones in the United States in [year]. Synoptic
weather times are typically set based on the Universal Time Constant, with times
recorded in Coordinated Universal Time (i.e., Greenwich ...), and time stamps on
weather maps and data based on this time zone often is given a "Z" (for "Zulu",
an earlier name for this timezone) after the time.

> "Accuracy in forecasting depended on the accurate timing of observations. At
first weather observers used local time because no system of standard time
existed, either in the United States or abroad. Local time often meant railroad
time, and that could vary as much as 20 minutes from the time proper to the
local meridian. Abbe called for change, emphasizing the need for simultaneous
readings by Signal Service observers in order to construct accurate charts and
weather maps. In 1875, he recommended to the president of the American
Meteorological Society the establishment of a committee on standard time. ...
Abbe's report in 1879 recommended the system of meridians that is now in general
use ... a system of time meridians one hour apart across the country, anchored
on the Greenwich prime meridian." [@willis2006cleveland]

> "From Abbe's perspective, however, the full potential for dynamic weather
prediction would not be achieved without standard time worldwide."
[@willis2006cleveland]

> "The time in all hydrometeorological products is expressed according to 
a single standard, which is the Universal Coordinated Time (UTC) 
(formerly known as Greenwich Mean Time [GMT] or Zulu (zero) Time [Z])."
[@usnwstime]

> "The U.S. National Weather Service base all data and product times on the 
UTC standard. ... A common practice is to include a time of the observation 
or event denoting the time with a following capital letter 'Z' to indicate the 
time as UTC." [@usnwstime]

> "The extent and the hour of the observations changed over time. Originally the
observers took the readings at 0735, 1635, and 2335, Washington time, but the
last reading was soon changed to 2300 so that the information could be included
in the morning newspapers. Before the introduction of standard time and time
zones within the United States, a confusing multiplicity of local times existed.
In 1870 there were over 100 such regional times. For the purposes of the
meteorological observations, observers used Washington time until 1885.
Thereafter the observers took their readings according to eastern time, or that
at the 75th meridian west of Greenwich, England. Observers also made local-time
readings from 1876 to 1881.[30]" [@raines1996getting]

> "Once the observers had gathered the weather data, the means of reporting and
disseminating it became most important. Like the Smithsonian, the chief signal
officer made arrangements with the leading commercial telegraph companies to
carry the tri-daily reports. Civilian telegraph experts established special
circuits routed to the Signal Office. The initial arrangement with Western Union
regarding transmission was only temporary, and at the end of the trial period
the company refused to continue service. The House Appropriations Committee held
hearings over the dispute and ruled that the company had a mandate to transmit
the weather information as government business. The Signal Corps compensated the
company, however, at rates determined by the postmaster general.[34]" [@raines1996getting]

> "When making the daily telegraphic reports, the weather observers used special
codes to reduce their length to twenty words in the morning and ten in each of
the other two reports, thereby saving the government both time and money.[35]
Regular transmission of the reports began at 0735 on 1 November 1870 from
twenty-four stations stretching from Boston, Massachusetts, south to Key West,
Florida, and west to Cheyenne in the Wyoming Territory. In addition to the
station atop Mount Washington (opened in December 1870), the Signal Corps soon
reached new heights in weather reporting with the station on Pikes Peak that
began reporting in November 1873.[36]" [@raines1996getting]

> "To provide a picture of weather conditions across the country, the observers
made their reports as nearly simultaneous as possible. The weather service did
not initially make forecasts, and the enabling legislation did not specifically
call for it to do so. Eventually general forecasts, referred to as
probabilities, emanated from the Signal Office in Washington. Locally, the
observers posted bulletins and maps in the offices of boards of trade and
chambers of commerce to provide weather information to the public. Post offices
also displayed daily bulletins, and observers supplied local newspapers with
data. Some communities appointed meteorological committees to confer with the
chief signal officer and to serve as a check upon the operations of the local
weather station. On the national level, the Signal Office in Washington issued
daily weather maps compiled from the reports received from all the stations. It
also published the Daily Weather Bulletin, Weekly Weather Chronicle, and the
Monthly Weather Review. All were available for sale to the public. Myer
estimated that through these various means at least one third of American
households received the Signal Corps' weather information in some form. A
railway bulletin service, initiated in 1879, enabled stations along many major
railroads to display weather information.[37]" [@raines1996getting]

> "The Signal Corps' telegraph network soon expanded beyond the eastern
seaboard. In 1874 Congress enacted legislation directing the War Department to
build lines to connect military posts and protect frontier settlements in Texas
against Indians and Mexicans. Other acts authorized lines in Arizona and New
Mexico and, somewhat later, the Northwest. These lines were intended to serve
sparsely settled areas where commercial lines were not yet available. For the
most part, soldiers maintained and operated the lines, but the Corps employed
some civilians. As the telegraph extended its reach, the weather system also
grew, because the operators doubled as weather observers." [@raines1996getting]

> "Until 1934 the Weather Bureau offices operated 12--15 hours a day with two
basic observations taken at 8 a.m. and 8 p.m. The observations were transmitted
via telegraph. There were no satellite images and few upper air observations."
\url{https://www.weather.gov/dvn/armistice_day_blizzard}

R3 Comment 5

This database serves as the basis for much of the large-scale (e.g., assessing
patterns over the US over long time periods) tornado-based research for the US. 
It does have some well-documented limitations. One is a notable increase over 
time in the number of tornadoes reported, particularly for the weakest, F0 [?]
tornadoes. [This stabilizes some in the age of radar?]

> "Weather events listed in the publication Storm Data are considered to be
the official database for tornadoes by the National Weather Service." 
[written by someone at the NWS Storm Prediction Center] [@mccarthy2003nws]

The US National Weather Service's Storm Prediction Center has a tornado database
called the "National Tornado Database" [@center2020storm]. This uses the NOAA
Storm Events database as its primary source [@center2020storm]. They note on their
website: 

> "The tables below provide the links to comma separated values (.csv) files for
tornadoes, hail, and damaging winds, as compiled from *NWS Storm Data*. Tornado 
reports exist back to 1950 while hail and damaging wind events date from 1955.


The NOAA Storm Events database, or the Storm Prediction Center's version of the
tornado database have been used in a number of studies, including in a number of
studies of tornado climatology [@tippett2016more; @brooks2003climatological;
@strader2015climatology].

> "The dataset we will use is the so-called smooth log of severe-weather reports
collected by the SPC and archived in the National Oceanic and Atmospheric 
Administration publication *Storm Data*." [@brooks2003climatological]

> "Data such as tornado magnitude, path length, path width, and so on are
recorded by the National Climatic Data Center (NCDC) as a public service and for
use in meteorological, climatological, and engineering studies (Edwards et al.
2013)." [@strader2015climatology]

Sometimes used in conjunction with other NWS data products: 

> "Initially, tornado cases available in the NWS's Damage Assessment Toolkit
(NOAA, 2014) were employed, which is a GIS-based framework for collecting,
storing, and retrieving damage survey data (Camp et al., 2014). This toolkit
provides a variety of tornado event filtering (tornado survey point, track,
footprint, and/or swath) and download options [key mark-up language (.kml) or
shapefile (.shp)], supplying an initial sample of georeferenced damage
assessments. In this study, a tornado footprint is defined as the maximum areal
extent of tornado intensity inferred by the damage, wind speed measured directly
by mobile Doppler radar, or assessed theoretically as the recorded length
multiplied with the maximum width as reported in Storm Data."
[@strader2015climatology]

> "To serve the public interest and the National Climatic Data Center (NCDC)
storm data record, the National Weather Service (NWS) documents the path length,
width, and maximum damage rating for every tornado county segment (NOAA
2007).[Footnote in manuscript: County segments of tornado paths are then
combined at the NWS Storm Prediction Center to yield a unified one-tornado
(ONETOR) dataset of whole-tornado records (Schaefer and Edwards 1999).] Such
data are used in meteorological and climatological research, as well as in
determining construction standard for critical infrastructure such as
high-tension electric lines and nuclear power plants (e.g., Ramsdell et al.
2007)." [@edwards2013tornado]

Sometimes used in conjunction with other datasets: 

> "The contiguous U.S. tornado fatality dataset utilized in this study was 
transcribed and compiled from two primary sources: (1) a long-term study of 
U.S. tornados by Grazulis (1993, 1997, hereafter Grazulis dataset) and 
(2) the National Climatic Data Center's Storm Data (NCDC 1959--2005) and 
'Storm Events' datavase. The Grazulis dataset included 'significant' tornado
events, including all events that are known to have produced a fatality, 
from 1680 to 1995. Storm Data reports from 1959 to 2005 were utilized to 
supplement the existing record of killer tornado events documented by 
Frazulis." [in a study of tornado fatalities] [@Ashley2007]

History: 

> "During the early 1950s, the US Weather Bureau began a concerted effort to
count all US tornadoes. In 1950, a list of tornadoes was provided in the
*Climatological Data National Summary*, but it was not until 1953, the first
full year of the issuance of Weather Bureau tornado watches, that the agency
began to formally count all tornadoes. The *Climaticalogical Data National
Summary* document evolved into *Storm Data* by 1959, which, in addition to
tornado data, included information on other storm perils, such as severe hail,
high winds, and floods (Grazulis 1993). Over the years, the tornado dataset has
been formatted and adjusted by the National Severe Storms Forecast Center and
Storm Prediction Center (SPC), and currently the National Tornado Database is
administered by the NWS Headquarters, SPC, and NCDC (McCarthy 2003)."
[@Ashley2007]
