---
title: "Response to reviewers' comments for *\"Assessing United States county-level exposure for research on tropical cyclones and human health\"*"
output: bookdown::pdf_document2
toc: false
bibliography: writing/hurr_exposure.bib
header-includes:
- \usepackage{xcolor}
- \usepackage{framed}
- \usepackage{bm}
- \usepackage{subcaption}
- \usepackage{gensymb}
- \renewcommand\familydefault{\sfdefault}
- \usepackage{sansmath}
- \usepackage{bm}
- \usepackage{soul}
- \sansmath
- \usepackage{booktabs}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{pdflscape}
- \usepackage{float}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{fullpage}
- \usepackage{pdflscape}
- \usepackage{tcolorbox}
- \tcbuselibrary{skins,breakable}
- \usepackage{amsmath}
- \usepackage{xtab}
- \usepackage{rotating}
- \allowdisplaybreaks
- \usepackage[nomarkers]{endfloat}
- \pagenumbering{gobble}
- \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
- \usepackage{longtable}
- \DeclareDelayedFloatFlavor{longtable}{table}
- \usepackage{pdflscape}
- \setlength\LTcapwidth{\textwidth} 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(knitr)
```


# Overview {-#overview}

\colorlet{shadecolor}{blue!8}
\begin{shaded}
\textbf{Associate Editor's Overview:}
Developing a consistent and comparable metric for exposure to tropical cyclones
would be a valuable addition to the literature. All reviewers had questions
about the exposure metric, including the challenges of using county-level data
where not everyone in a county would have been exposed and the difficulties of
going from country-level to individual-level risks. Further the comparison of
different storm hazards could be clarified. The reviewers identified several
issues where further explanation would increase the accessibility of the
manuscript.
\end{shaded}

Thank you for the opportunity to revise and resubmit this manuscript. We
appreciate the helpful suggestions from the reviewers for clarifying and
deepening the discussion in the paper. 

Broadly, we have: 

1. Added discussion on the question of county-level versus individual-level
data. 
2. Added explanations for how our comparisons among different storm hazards
can help epidemiologists in designing studies and statistical analysis.
3. Added a small analysis of the correlation in rainfall metrics specifically
in cases of high rainfall ($\ge75$ mm of cumulative rainfall in the county
associated with the storm), to deepen the discussion of potential disagreement
in continuous measures of rainfall during extreme conditions. 
4. Changed labeling, axis range, and other presentation details for some
figures, as well as confirmed that the color scales will be accessible.
5. Corrected typos and improved wording throughout based on suggestions of the
reviewers.

For figures and tables that were changed in the revision, we have reproduced them 
at the end of this response, as well as some figures showing how figures will be
perceived for those who are colorblind and when printed in grayscale.

\medskip

# Reviewer 1: {-#reviewer-1:}

## Overall comments {-#overall-comments}
\colorlet{shadecolor}{blue!8}
\begin{shaded}
\textbf{Reviewer 1 Overview:}
I applaud the authors for this work, and their efforts to develop standardized
ways to measure exposure to tropical cyclones. As a disaster scientist, I very
much appreciate efforts that promote our collective ability to learn from and
across disasters to build cumulative science. 
\end{shaded}

**Response:** Thank you for your helpful suggestions. We have responded to 
each in detail below.

\medskip

\begin{shaded}
\textbf{Reviewer 1 (R1) Comment 1:}
I very much appreciate the rationale for authors' decision to focus at the
county level, especially given the available health-related data. But, since
tropical cyclones don't necessarily have consistent impacts across an entire
county, I think the limitations of this decision are worthy of further
discussion in the discussion section.
\end{shaded}

**Response:** 

This is an excellent point. We have added text in the Limitations section of the
Discussion on the potential limitations of using aggregated, county-level
exposure data in a research study:

> **Text added to Discussion (added text in bold):** 
"Second, these data are aggregated to the county level. **This spatial scale
allows for easy integration with health outcome data aggregated at the county
level. Such data is often used for disaster epidemiology, as aggregated data may
be easier to access than individual-level data, especially at a scale that
covers many locations and years and so allows higher statistical power and
includes a broader range of exposure levels [@wakefield2008overcoming].**

> "**Such ecological exposure assessment, however, sets a common exposure level
throughout the county, ignoring within-county variability, even though such
variability exists. For some hazards, this within-county variation could be
stark. For example, tornadoes cause very localized damage, directly along the
tornado's path---a tornado can destroy homes on one side of a street while
leaving those on the opposite side untouched. Levels of other hazards, like
storm-associated winds and rain, will also vary within a county, but typically
with smoother variation. In particular, it will be unlikely that a county will
have one area that is exposed to extremes of these hazards while other parts of
the county are completely unexposed, as both the wind fields and rain fields of
tropical cyclones tend to be large in comparison to the size of a county.**

> "**Aggregated data can be used to infer contextual-level associations---for
example, the association between county-level exposure to a storm hazard and
county-wide rates of a health outcome. However, ecological data is also
sometimes used to infer individual-level associations (e.g., the association
between personal exposure to a storm hazard and personal risk of  experiencing
the health outcome). Individual-level inference from ecological/aggregated data
is susceptible to ecological bias [@greenland1994invited; @portnov2007ecological;
@idrovo2011three]. Researchers who use the data provided here for ecological
studies---with the aim of making individual-level inferences---should be aware
of this potential and could explore approaches for minimizing risk of ecological
bias (e.g., @wakefield2008overcoming).**"

\medskip

\begin{shaded}
\textbf{R1 Comment 2:}
Are there opportunities to expand this approach internationally? Some decisions
(e.g., county-level) data may restrict cross-country comparisons. Can the
authors discuss this? 
\end{shaded}

**Response:** 

While there would be some limitations in comparisons based on differences in the
scale of county-equivalents, many countries affected by tropical cyclones do
have geopolitical areas that are fairly comparable to US counties, like
municipalities in Mexico and districts in India. There would be a few challenges,
however, in extending the exposure dataset for the hazards that we present in
this paper. We have added text to the paper's Discussion on opportunities and
barriers to expanding this approach internationally:

> Text added to the Discussion (added text in bold): 
"**This dataset is limited to the contiguous US, and while expansion to global
coverage would be useful and feasible, there would be some challenges. For
precipitation data, the re-analysis product used here (NLDAS-2) only covers the
contiguous US, although other re-analysis products, as well as other types of
precipitation datasets, have global coverage [@sun2018review]. The wind data are
generated based on a model that is currently US-focused [@stormwindmodel], but
could be extended to other areas, although this would require adding new
land/sea masks within the associated software, as well as accounting for
differences across storm basins in wind averaging periods
[@harper2010guidelines] and in the direction of cyclonic winds in the Northern
versus Southern Hemisphere. Further, since the core of the wind model was
developed based on data from Atlantic-basin storms [@willoughby2006parametric],
an extension to other areas should include separate validation and calibration
to ensure it performs appropriately in those settings. Alternatively, other wind
field modeling software is available that provides a global coverage, like
Geoscience Australia's Tropical Cyclone Risk Model
(http://geoscienceaustralia.github.io/tcrm/). For tornado and flood events, the
data described in this paper drew on a US-focused storm events database, and so
international extension of data on these hazards would require access to similar
databases covering other countries. Finally, the relevant geopolitical
boundaries to use for aggregation would vary by country (e.g., municipalities in
Mexico, districts in India).**"

\medskip

\begin{shaded}
\textbf{R1 Comment 3:}
In non-health fields, a county's inclusion in a FEMA major disaster declaration
or eligibility for public/individual assistance (all of which are somewhat
reflective of level of damage) is sometimes used as an "exposure" proxy. Have
you considered if/how your exposure assessments relate to inclusion in a FEMA
disaster declaration? In other words, does exposure to any of the hazards align
with the level of damage required for a county to be considered a "disaster
area" and/or worthy of federal assistance by current federal policy? Does this
matter?
\end{shaded}

**Response:** 

This is a great question. For tropical cyclones, some health impacts will come
directly from physical exposures like wind, rain, and flooding. However, many of
the impacts might come through indirect pathways, which start from physical
hazards but move through pathways related to property damage, infrastructure
damage, access to typical medical care, mold growth, and a number of other
intermediates. Ultimately, it would be useful to have large-scale databases of
these secondary exposure pathways to use for more complex studies of the
pathways between the physical hazards of these storms and how they affect human
health. In this sense, something like the FEMA declarations could be considered
not as a proxy for assessing exposure to physical hazards, but rather as a proxy
of damage caused by the storm, and so a measurement informative for one of these
indirect pathways from physical hazards of the storm to health impacts, through
something like a mediation analysis.

However, there are some limitations to the FEMA disaster declarations in terms
of using them as a proxy for the amount of damage a storm caused in a county.
FEMA's disaster declarations are issued to provide assistance, based on damage
assessment, to individuals or the entire public in certain locations where
catastrophes overwhelm the state or local government [@mccarthy2014fema], and
any use of them for exposure assessment is secondary. Due to the political
nature of disaster declaration process, these declarations are often subject to
many political and economic factors [@mccarthy2014fema; @logue1981research].
Given the political and economic nature of these declarations, there is likely
variation across time and geography in the likelihood of a given exposure
resulting in a disaster declaration. This could be problematic for a
study that seeks to explore risk across multiple years and affected communities.

Other sources of data may therefore be better suited to use as a marker of
damage to assess that pathway of indirect health impacts from a tropical
cyclone. For example, data on storm impacts, including property damage or
insurance claims data, are sometimes available through storm databases or
publications (e.g., NOAA's Storm Events data, tropical cyclone reports published
in the *Monthly Weather Review*). While such data could potentially be used to
create an exposure metric, tropical storm damage data requires some
normalization, to incorporate both changes in dollar value over time and also
changes in development in at-risk areas, to be comparable over extended time
periods [@pielke1998].

We have added text discussing this point to the Discussion:

> **Text in revised manuscript (added text in bold):**
"**The dataset we present focuses on the physical hazards of a tropical cyclone.
However, health impacts will often come through indirect pathways, including
through damage to property and infrastructure. In future work, it would be
useful to expand this dataset to add data related to these pathways, to allow
research exploring the role of indirect pathways from tropical cyclone phyiscal
hazards to health risk. This expansion could include adding data on normalized
storm-associated damages, or proxy measurements of such damages, from sources
like the US NOAA's Storm Events database and the US Federal Emergency Management
Agency's county-level disaster declarations.**"

\medskip

\begin{shaded}
\textbf{R1 Comment 4:}
Line 95---wondering if there is a better word for "hits" here. Maybe "exposed?"

\textit{Lines 92--95 in the original manuscript:}

\begin{quotation}
\noindent
\textit{"In many cases, these studies analyze multi-year, multi-community data,
allowing them to estimate average associations over many disasters and to
explore how a disaster's characteristics, or the characteristics of the
community it hits, modify associated health risks (e.g., Anderson and Bell 2010;
Son et al. 2012; Liu et al. 2017)."}
\end{quotation}
\end{shaded}

**Response:** 

Thank you for this suggestion. We have changed the wording in this sentence: 

> **Text in revised manuscript (relevant change in bold):**
"In many cases, these studies analyze multi-year, multi-community data, allowing
them to estimate average associations over many disasters and to explore how a
disaster's characteristics, or the characteristics of the **affected
communities**, modify associated health risks (e.g., Anderson and Bell 2010; Son
et al. 2012; Liu et al. 2017)."

We also used the word "Hits per county per decade" in the legend for Figure 5 in
the main text (reproduced at the end of this response document as Figure
\@ref(fig:fig5)) and Figure S3 of the Supplemental Material. We have revised the
wording in these figures to "Exposures per county per decade".

```{r fig5, fig.cap="Reproduction of Figure 5 from the original manuscript (left) and revised version in the resubmitted manuscript (right). In the revised version, the figure legend was changed from '\\textbf{Hits} per county per decade' to '\\textbf{Exposures} per county per decade' (bold used to highlight change). Figure caption in the revised manuscript: \\textit{Figure caption.}", fig.show = "hold", out.width="40%", fig.align = "center"}
include_graphics("../figures/averageexposureonly.pdf")
include_graphics("figures/averageexposureonly.pdf")
```

\medskip

\begin{shaded}
\textbf{R1 Comment 5:}
Lines 137--139, suggest breaking into two sentences

\textit{Lines 137--141 from original manuscript:}

\begin{quotation}
\noindent
\textit{"While other approaches have been developed to incorporate storm
hazards, particularly wind, into exposure assessment (e.g., Grabich et al.
2015a; Zandbergen 2009; Czajkowski et al. 2011), there is not yet a standard
approach, and when different studies use different datasets or storm hazards
when assessing storm exposure, it becomes difficult to compare and aggregate
findings."}
\end{quotation}
\end{shaded}

**Response:** 

This is a great suggestion. We have split the cited sentence into two sentences:

> **Text in revised manuscript:**
"While other approaches have been developed to incorporate storm
hazards, particularly wind, into exposure assessment (e.g., Grabich et al.
2015a; Zandbergen 2009; Czajkowski et al. 2011), there is not yet a standard
approach. When different studies use different datasets or storm hazards
when assessing storm exposure, it becomes difficult to compare and aggregate
findings."

\medskip

\begin{shaded}
\textbf{R1 Comment 6:}
This is not a comment on the paper, but the authors may also want to consider
publishing their work in DesignSafe, which is "the web-based research platform
of the NHERI  [NSF-Funded Natural Hazards Engineering Research Infrastructure]
Network that provides the computational tools needed to manage, analyze, and
understand critical data for natural hazards research." It is the data
repository widely used by interdisciplinary hazards and disaster researchers.
\end{shaded}

**Response:** 

It turns out that the data we present should already be accessible to use on the
DesignSafe platform, because we have shared the data as an R package.

DesignSafe is an online platform sponsored by the National Science Foundation as
part of the Natural Hazards Engineering Research Infrastructure
[@rathje2017designsafe]. This platform can be used to store, share, and publish
citable data related to natural hazards through its Data Depot. It goes beyond a
data repository, however, and also allows researchers to conduct data
integration and analysis in the cloud through its Discovery Workspace, while
drawing on data stored in the Data Depot and while using scientific programming
tools. As part of its platform, DesignSafe allows users to work in a number of
programming languages, including R (through Jupyter Notebooks)
[@rathje2017designsafe]. Therefore, DesignSafe users already have access to any
data shared through an R package, including the data we present in this
manuscript, without the data needing to be separately loaded and published
through DesignSafe's Data Depot.

This is great, because it might otherwise be difficult to try to maintain copies
of the dataset in different repositories (i.e., one copy as an R package and
another as a DesignSafe dataset), and there could be confusion about things like
the equivalence of the two copies. R packages are citable through a standard
format that gives the R package version, while data publicly shared on
DesignSafe is assigned a digital object identifiers (DOI)
[@rathje2017designsafe], and so different citations would be used for the same
dataset depending on how the user accessed it. If updates of one copy were not
immediately updated for the other copy, the copies could get out of sync, and
maintaining two synced copies would add would add an extra layer of work with
each update to the R package (we have been updating several hazards as new data
becomes available). DesignSafe's interface with R avoids this problem by
allowing its users direct access to our package and its data through that path,
as it allows for a single, citable, licensed version of the data stored and
shared from a single location, with no question of syncing separate versions of
the data or clarifying licensing and citation practices across two storage
locations.

We have added a note in the manuscript to let readers know that, because the
data is shared through an R package, and because DesignSafe's Discovery
Workspace allows users to work in R, the data can be fully accessed and used for
those working on the DesignSafe platform by using the same code in a DesignSafe
Jupyter notebook that would otherwise be used on a local computer.

> **Text added to the Methods of the revised manuscript (added text in bold):**
"We have posted most study data as an open-source R package (Anderson et al.
2020). **By sharing this data as an R package, it is also accessible through
cloud-based computing platforms that incorporate Jupyter notebooks, including
the National Science Foundations's DesignSafe platform for natural hazards
engineering research [@rathje2017designsafe].**"

\medskip

# Reviewer 2: {-#reviewer-2}

\begin{shaded}
\textbf{Reviewer 2 overview:}
Exposure assessment is essential to study health impacts from disasters, but it
is also a great source of biases due to a lack of a good and fine measure of
exposures during the chaos. Often, epidemiologists use existing data at large
population level such as county level weather data which provide no individual
level exposures but give average exposure at the population level. So it is very
challenging to draw a good causal inference to health outcomes that measured at
the individual level. This manuscript is a well-written report with generously
shared data and programs on US tropical cyclones. This will contribute
significantly to disaster epidemiology which is still a new area and suffering
from good exposure ascertainment.

While I enjoyed reading the manuscript very much, I have one question I could
not figure out clearly.

One objective is to investigate patterns and agreement between different
exposure measures. I do not clearly understand the value of studying agreement
between exposures, while each exposure should be measured separately, so both
individual and overall effects can be assessed.  I do understand using "distance
from the storm" as exposure is only proxy measure and highly sensitive to
misclassification, but because direct storm hazards such as rainfall, flood,
storm surge, and tornado are publicly accessible as authors indicated, authors
could just simply guide readers to use the best exposure data rather than
conducting a quite intense analysis. This may be due to my limited knowledge, so
it would be appreciated if the authors provide explicit explanations on the
purpose and benefit of the analysis.

\end{shaded}

**Response:** 

Thank you for this suggestion, and we can see that we could have more clearly 
articulated in the original manuscript how our assessment of patterns in and 
among exposures to specific hazards can help epidemiologists to design their
studies and the statistical methods to analyze the resulting data. We provide
a detailed response in our response below to Comment 1 from this reviewer.

We have moved suggestions that the reviewer sent in "Track Changes" of the
Word document for the original manuscript and address each in detail below. When
there were several comments on a related topic, we have grouped them together to
provide a more cohesive response to the point.

\medskip

\begin{shaded}
\textbf{Reviewer 2 (R2) Comment 1:}

Several of the comments added by the reviewer in the Word document focused on the
purpose of calculating agreement among hazard-based metrics. These were:

\begin{itemize}
\item Line 164: Isn't it obvious to use better exposure measures than proxies?
Wind, rainfall, tornado, and flood are direct products of storms while distance
is only proxy of those exposures. Rather than calculating agreement among the
exposures, it would be important to note that these are independent hazards that
can come together or come with different intensity and at different time
windows.
\item Line 354: I am not sure why different exposures should be compared and
measured the agreement. I would just treat each exposure as single independent
exposure and give a summary score by summing all the exposure with some
weighting for multiple exposures. I may not understand the importance of this
approach. If so, adding the explanation would help readers not to questioning.
\item Subsection of Results on \textit{Agreement across exposure metrics}: 
As stated below, storms bring different hazards to different locations with
different time. What would be new learning by comparing these different pairs of
exposure metrics. Somewhat apple and orange comparison. Maybe providing more
explanation can help what the important learning from this is. I may not catch
it. Similar comment was added earlier.
\item Lines 494--495: Why this agreement is important? Same questions added above
already, but still wonder what I missed. It seems like the authors try to
develop an exposure matrix that is common in occupational epidemiology, but it
is only valid method when the exposures are similar and lead to the same health
outcomes.  Rain, wind, flood, and tornado should be treated separately, maybe
except rain and flood. It is possible to be an extreme storm with only severe
wind without major impacts from other exposures. Tornado is a good example.
\end{itemize}

The text in the manuscript for comments with line numbers are reproduced below: 

\textit{Lines 164--166 from original manuscript:}

\begin{quotation}
\noindent
\textit{"They found important differences, concluding that a study may be prone
to bias from exposure misclassification if distance to the storm track is used
as a proxy for exposure (Grabich et al. 2015a)."}
\end{quotation}

\textit{Line 354 from original manuscript:}

\begin{quotation}
\noindent
\textit{"Finally, we investigated how well exposure assessment agreed across
these metrics."}
\end{quotation}

\textit{Line 491--495 from original manuscript:}

\begin{quotation}
\noindent
\textit{"For another set of tropical cyclones (e.g., Ernesto in 2006, and Bertha
in 1996, Isabel in 2003), there was moderate to good agreement for pairwise
combinations of distance, rain, and wind, but poor agreement for other
combinations of metrics, while for another set of storms (e.g., Matthew in 2004
and Katrina in 2005), there was moderate to good agreement between distance and
rain."}
\end{quotation}

\end{shaded}

**Response:** 

We have included a measurement of agreement between different hazard-based
exposure metrics for two reasons. First, while we agree with the reviewer that a
direct measurement of each relevant hazard is preferable to a proxy, there are still some studies
that, when assessing tropical cyclone exposure based on measurements of physical
hazards, use a single hazard for this exposure assessment. We are most familiar
with cases where a wind-based metric has been the focus (e.g.,
@sun2020tropical, @grabich2015measuring). While this may not be 
ideal, it is done in practice. To interpret these studies, it is helpful to
understand the degree to which this single-hazard metric agrees with exposure
patterns for other hazards. If the exposure patterns are in strong agreement for
all hazards, then these studies could be interpreted as estimating risk
associated with exposure to tropical cyclones as a whole. If the exposure
patterns are in low to moderate agreement, then these studies should be
interpreted instead as estimating the risk associated with that specific storm
hazard, but not for all storm hazards. In this case, if the agreement is
moderate, there is a chance of confounding from other storm hazards
(i.e., an association that is attributed to one storm hazard was actually the
result of another that was not included in the model). In this paper, we present
measurements of agreement across four hazard-based exposure metrics that can
provide insights on these points.

Second (and related), epidemiologists need to understand how these exposure
measurements agree to help in planning studies and statistical approaches to
interpreting the resulting data. In this sense, "agreement" is providing a
measure similar to correlation for continuous variables; our choice of agreement
with the binary exposure classifications reflects the more common use of binary
classifications for most tropical cyclone epidemiology to date.

There are several things that an epidemiologist could derive from these results
to help in designing a study and the associated statistical analysis. First, the
results we present can help epidemiologists determine if they can use a single
metric of tropical cyclone exposure in their studies, or if they need to instead
consider tropical cyclones as a mixture of hazards. There are a number of
practical reasons to use a single exposure metric, if doing so is reasonable
(even if it's not conceptually ideal). While there have been advances in methods
for mixtures research in environmental epidemiology, these are often more
complex than the long-used and well-developed methods for studying single
exposures, and many epidemiologists are much more comfortable with methods for
single exposures compared to methods for mixtures. Further, it can be easier to
communicate results from single-exposure studies clearly and succinctly.
Therefore, if there is a reasonable way to measure exposure with a single
metric, there are practical reasons that recommend that approach, even if
conceptually it might be preferable to incorporate separate hazards. 

If, at the county level, there were strong agreement among exposure patterns for
the different tropical cyclone hazards we considered, it might have been
reasonable to recommend the use of a single hazard-based metric as a proxy for
exposure to all storm hazards in epidemiological studies for practical reasons.
We found that this is almost never the case, although there are a few select
storms (like Hurricane Floyd in 1999) where such an approach might be
justifiable. Most atmospheric scientists would not be surprised by our findings
about this disagreement in exposure patterns for different tropical cyclone
hazards, but this finding is not as self-evident for environmental
epidemiologists. Based on a recent conference of the International Society for
Environmental Epidemiology, for example, there are a number of environmental
epidemiologists with backgrounds focused on methods and other ambient exposures
(e.g., air pollution, heat) who are beginning to include tropical cyclones as an
exposure of interest in their studies, and we believe these results would be
novel and useful to many of them.

Further, in cases where it is clear that a mixture approach is necessary, these
results can help in understanding what challenges might arise in statistical
analysis. While one approach to including several hazards could be to create an
index of tropical cyclone exposure by taking a weighted sum of all hazards, for
most health outcomes it is unclear *a priori* what the weight for each hazard
should be in such an index, and so it will often be preferable to discover those
weights from the data by fitting a statistical model that includes each hazard
as an independent variable. In these cases, strong correlation (or agreement, in
the case of binary exposures) among separate independent variables can create
both numerical issues (e.g., variance inflation from multicollinearity) and
difficulties in interpretation (e.g., difficulty inferring the role of specific
hazards and separating the influence of each). If an epidemiologist has a good
understanding of these patterns in the data, he or she can adopt methods that
address potential issues, including using matching-based study designs to create
a subset of storms in which separate hazards are well-distinguished,
incorporation of dimension-reduction methods like principal
components analysis, or shrinkage techniques that allow for a small amount of
bias in estimation in return for reducing problems from
collinearity (e.g., ridge regression) [@leal2012multicollinearity;
@schisterman2017collinearity].

The reviewer makes a good point that it is important for us to clarify our
motivation for measuring agreement between the hazard-based metrics, as well as
to show readers how our results from these measures can be used to help in
designing studies and planning statistical analysis for tropical cyclone
epidemiology. We have added text to the manuscript on these points: 

> **Text in revised manuscript (relevant added text in bold):** 
"Finally, we investigated how well exposure assessment agreed across these
metrics. **These results can help epidemiologists answer several key
questions as they interpret previous research and design new studies. For
example, if a single storm hazard has been used to measure exposure in an
epidemiological study, can the result be interpreted as an association with
tropical cyclone exposure in general, or should it be limited to represent an
association with that specific storm hazard? If a study investigates the
association between a single storm hazard and a health outcome, could this
estimate be confounded by other storm hazards? When planning a new study that
will incorporate several storm hazards, might there be variance inflation from
multicollinearity or difficulties in disentangling the roles of separate
hazards?**"

\medskip

\begin{shaded}
\textbf{R2 Comment 2:}

Other comments were related to correlations estimated in the validation 
analysis for the precipitation data:

\begin{itemize}
\item Line 394: It looks like the correlation coefficients can be very low if measured
from 75mm or somewhere around.  This seems to be a significant issue for
analyzing storms with heavier rainfall using NLDAS-2 data.
\item Line 515: It would be informative to provide correlation coefficient for only
above 75 mm measures.
\end{itemize}

The text from the referenced lines are reproduced below:

\textit{Line 392--394 from original manuscript:}

\begin{quotation}
\noindent
\textit{"Within these counties, storm-related rainfall measurements were
well-correlated between the two data sources, with rank correlations (bottom
right of each graph in Figure 2) between 0.87 and 0.98. "}
\end{quotation}

\textit{Line 514--515 from original manuscript:}

\begin{quotation}
\noindent
\textit{"The rainfall data are generally well-correlated with ground-based
observations, but may sometimes underestimate very high rainfall values (Figure
2)."}
\end{quotation}
\end{shaded}

**Response:** 

This is a great suggestion. In the original manuscript, we describe this lower
correlation at higher precipitation values through the use of examples from
specific storms, and discuss the potential reasons for it. Measuring the
correlation specifically within these higher-precipitation events would add more
quantitative evidence to this discussion. We have added estimates of these
correlations specifically for high-precipitation events in a new table in the
Supplemental Material (Table S2 of the manuscript, reproduced as Table
\@ref(tab:highprecipcorr) at the end of this response document).

\input{tables/precip_high_corr.tex}

In several cases, the correlation was much weaker when limiting analysis to
these severe rainfall events. For example, in Miami-Dade County, FL, the
Spearman correlation coefficient was 0.94 for all 64 tropical cyclones that came
near the county over the analysis period, while it was only 0.49 among the 18
storms with cumulative precipitation of 75 mm or more. There was a similar
reduction in Mobile County, AL. However, for some counties, the correlation
remained high (e.g., Harris County, TX; Orleans Parish, LA), although in all
cases there are few extreme precipitation events (5--20 across
these sample counties), and so there is likely substantial random variation in
these correlation estimates, since they are based on small numbers of 
observations.

We have added text to mention this point and to direct readers to the added
supplemental table:

> **Text in revised Results (relevant addition in bold):**
"The rainfall data are generally well-correlated with ground-based
observations, but may sometimes underestimate very high rainfall values (Figure
2). **Further, in some counties, the correlation was substantially lower when 
considering only tropical cyclones with cumulative local rainfall of $\ge75$ mm
(Supplemental Table 2)**."

> **Text in revised Discussion (relevant addition in bold):**
"The rainfall data are generally well-correlated with ground-based observations,
but may sometimes underestimate very high rainfall values (Figure 2), **and in
some counties the correlation was substantially lower when considering only
tropical cyclones with cumulative local rainfall of $\ge75$ mm (Supplemental
Table 2).**"

\medskip

\begin{shaded}
\textbf{R2 Comment 3:}
Figure 4: I would add Y-axis label "NOAA classification."
\end{shaded}

**Response:** 

This is a great suggestion. We have made this change, as well as several changes
in response to EHP's Request 4 later in this response. The original and revised
versions of Figure 4 of the manuscript are reproduced in Figure \@ref(fig:fig4)
at the end of this response.

```{r fig4, fig.cap="Reproduction of Figure 4 from the original manuscript (top) and revised version in the resubmitted manuscript (bottom). In the revised version, we have added a title to the y-axis ('Classification based on NOAA Storm Events Data'), binned the values shown by each point's fill, so these colors are easier to distinguish, increased the size of the points, and changed the algorithm for jittering, so none of the points overlap.", fig.show = "hold", fig.align = "center", out.width="100%"}
include_graphics("../figures/floodcomparison.pdf")
include_graphics("figures/floodcomparison.pdf")
```

\medskip

\begin{shaded}
\textbf{R2 Comment 4:}
Table 2: This is another confusing table. Not sure what compared here. The
second column is a mean \# of exposed counties over years and the third column
seems like showing single storm with the max \# of exposed counties. Not sure
what info I should digest from this table.
\end{shaded}

**Response:** 

We included this table with the intent that these results can help
epidemiologists who are planning new single-storm or multi-year studies of
tropical cyclones and associated health risks. For planning a multi-year study,
it is helpful to know how many exposures the study might incorporate, as this
can help in giving a general idea of whether the study might have adequate
statistical power to investigate a certain research question. Similarly, for
single-storm studies, it is helpful to know how widespread exposure can be for a
specific hazard during a very large storm. This table provides some basic
summaries of the extent of exposure to different tropical cyclone hazards, both
on average for the storms in the study and also for those storms that were most
extensive.

We have added text to help the readers interpret this table:

> **Text from Results of revised manuscript (added text in bold):**
"**Table 2 provides summary statistics describing the extent of counties
assessed as exposed based on specific storm hazards, to help epidemiologists in
planning studies, including understanding the potential statistical power to
study specific exposures, both for multi-storm and single-storm studies.**
Across the four storm hazards considered, there was wide variation in the
average number of county exposures per year. For tropical cyclone
tornadoes, there were on average about 40 county exposures per year within our
study. County exposures were more frequent for tropical cyclone wind exposures
(>160/year on average), even more frequent for tropical cyclone flood exposures
(>190/year on average), and most frequent for tropical cyclone rain exposure
(>290/year on average). For every hazard except tornadoes, we identified at
least one tropical cyclone that exposed over 250 counties (Table 2). However,
the largest-extent tropical cyclone varied across hazards: Frances in 2004
exposed the most counties based on rain, Michael in 2018 based on wind, and Ivan
in 2004 based on flooding and tornadoes (Table 2)."

\medskip

\begin{shaded}
\textbf{R2 Comment 5:}
Line 602: This is not a new finding, but how much misclassified would be a good
question to answer.

\textit{Line 600--602 from original manuscript:}

\begin{quotation}
\noindent
\textit{"Based on our results, the use of a distance-based metric to assess
exposure to any of these hazards, or the use of measurements from one hazard as
a proxy for exposure to any of the other hazards considered, would often
introduce exposure misclassification."}
\end{quotation}
\end{shaded}

**Response:** 

Figure 7 of the main text is meant to help illustrate the potential for exposure
misclassification, including when using a distance-based metric as a proxy. We
used the Jaccard index because we are using data for all eastern US counties.
For any storm, there will be large parts of this area that are
unquestionably unaffected, because they are so far from the storm. For example,
during Hurricane Floyd in 1999, the storm made landfall in North Carolina and
moved northward along the Eastern Seaboard. While there will be questions in
some cases about whether counties in the Mid-Atlantic and New England were
exposed to this storm, counties in Texas, Louisiana, and other states in the
west of the study area were unquestionably unexposed to this storm. If we
measured misclassification using a metric that includes these counties in the
denominator, it would be overly optimistic because of the strong agreement in
counties far from the storm, like Texas counties during Hurricane Floyd, and
wouldn't help to pinpoint the potential for disagreement in areas nearer the
storm path.

We therefore used the Jaccard index, which focuses on counties that were
assessed as exposed based on at least one of the two metrics being compared.
However, one downside of this metric is that it does not help readers understand
the direction of that disagreement. For example, it does not provide information
about how many counties were classified as "exposed" based on a distance proxy
but "unexposed" by a hazard-based proxy. This could be useful in interpreting
our results in Figure 7 of the main table.

\input{tables/wind_misclass.tex}

\input{tables/rain_misclass.tex}

\input{tables/flood_misclass.tex}

\input{tables/tornado_misclass.tex}

We have added four new tables in the Supplemental Material (Tables S3--S6), one
for each of the hazard-based metrics we considered. We have reproduced these
tables at the end of this response document as Tables
\@ref(tab:misclasswind)--\@ref(tab:misclassflood). Each breaks down agreement
and disagreement between the distance-based proxy metric and one hazard-based
metric, focusing on storms with at least 200 counties exposed by at least one of
the metrics considered. These tables allow readers both to investigate general
patterns in disagreement and exposure misclassification across these storms and
also to see cases of unusual storms. For example, for most storms, disagreement
between the distance proxy metric and the wind-based metric resulted mainly from
mis-classification of counties as exposed based on distance while they did not
experience gale-force winds. However, there were a few storms, like Hurricane
Bertha in 1996, where the inverse was the case, and a distance-based metric
would have missed a large number of counties that experienced gale-force winds
from the storm (misclassified them as unexposed), but not misclassified many
counties as exposed when they in fact did not experience winds at this level.

We have added a reference to these new tables in the main text of the manuscript: 

> **Text in the revised manuscript (relevant addition in bold):** 
"We drew similar conclusions when we investigated all 46 tropical cyclones
between 1996 and 2011 (when data for all five metrics were available) for
which~100 or more counties were exposed based on at least one metric (Figure7;
**for the most extensive of these, storms for which 200 of more counties were
exposed based on at least one metric, Tables S3--S6 provide underlying numbers
comparing exposure assessments between the distance-based proxy and each
hazard-based metric**)."

> **Text in the revised manuscript (relevant addition in bold):**
"Based on our results, the use of a distance-based metric to assess exposure to
any of these hazards, or the use of measurements from one hazard as a proxy for
exposure to any of the other hazards considered, would often introduce exposure
misclassification **(Figure 7, Tables S3--S6)**."

\medskip

\begin{shaded}
\textbf{R2 Comment 6:}
Lines 626--627: It would still be beneficial to treat each exposure as a single
variable first and then consider overall impact. Only if each exposure data are
available.

\textit{Line 626--628 from original manuscript:}

\begin{quotation}
\noindent
\textit{"For these storms, it may be possible to assess exposure to multiple
hazards of the storm using a single metric, perhaps even a proxy like the
distance between the county and the storm’s track."}
\end{quotation}
\end{shaded}

**Response:** 
We agree. This type of approach has not always been easy in the past, but can
be easily implemented with the dataset we publish here. We have added to the
text on this point: 

> Text in revised manuscript (relevant addition in bold):
"For these storms, it may be possible to assess exposure to multiple
hazards of the storm using a single metric, perhaps even a proxy like the
distance between the county and the storm’s track. **With the dataset we 
describe in this paper, however, there is little need to limit analysis
based on exposure to a single hazard or proxy, although multi-hazard studies
of storms with high agreement among hazard exposures should look out for 
modeling issues from multicollinearity.**"

\medskip

# Reviewer 3: {-#reviewer-3}

\begin{shaded}
\textbf{Reviewer 3 overview:}
This paper describes the development of a dataset and R package that can be used
to assess exposure to tropical cyclone-related hazards (e.g., wind,
precipitation, flooding) at the county level in the eastern United States, as
well as a descriptive analysis of county-level exposure to these hazards. This R
package is novel and I believe of high interest to researchers interested in the
health effects of tropical cyclones.  I have a few minor suggestions to further
improve the clarity of the manuscript.
\end{shaded}

**Response:** 

Thank you for the helpful suggestions and recommendations. We have responded in 
detail to each comment or question below.

\medskip

\begin{shaded}
\textbf{Reviewer 3 (R3) Comment 1:}
Starting on line 128, the authors write "Extreme winds are more common to the
track's right, where counterclockwise cyclonic winds move in concert with the
tropical cyclone's forward motion". I think this statement may be specific to
tropical cyclones in the Northern hemisphere (i.e., those relevant to exposure
in the US). Would be good to clarify this.
\end{shaded}

**Response:** 

You are right, the opposite would be true in the Southern hemisphere. This
difference is driven by the Coriolis effect on cyclonic rotation, in which the
rotation of the earth diverts winds as they move toward the low pressure center
of the cyclone. We have edited the text to specify that this statement is
specific to the Northern Hemisphere:

> **Text in revised manuscript (relevant addition in bold):**
"**In the Northern Hemisphere, cyclonic winds are counterclockwise,** so
extreme winds are more common to the track's right where cyclonic winds move in
concert with the tropical cyclone's forward motion (Halverson 2015)."

\medskip

\begin{shaded}
\textbf{R3 Comment 2:}
Line 186: What is meant by the phrase "synoptic times"?

\textit{Lines 185--187 from original manuscript:}

\begin{quotation}
\noindent
\textit{"We used tracking data from HURDAT2, which records the storm center’s
position at the synoptic times of 6:00 am, 12:00 pm, 6:00 pm, and 12:00 am
Coordinated Universal Time (UTC)."}
\end{quotation}
\end{shaded}

**Response:** 

Some weather data are collected at standardized times, so that observations from
different locations can be meaningfully integrated to provide a view of
large-scale (synoptic) weather systems and patterns [@usnwstime;
@willoughby2007hurricane; @willis2006cleveland; @raines1996getting]. This
practice extends back over 100 years, when weather data for the US were
collected by local weather offices and sent by telegram at regular times each
day into a central office, which used this information to create synoptic
weather maps that captured major weather systems in the county and then
sent back forecasts to local weather offices, also at standardized times
[@willis2006cleveland; @raines1996getting]. Synoptic weather times are typically
set based on the Universal Time Constant, with times recorded in Coordinated
Universal Time (i.e., Greenwich Mean Time), and time stamps on weather maps and
data based on this time zone often is given a "Z" (for "Zulu", an earlier name
for this timezone) after the time [@usnwstime;
@morris2008time].

We have added text to clarify the meaning of "synoptic times" in the text:

> **Text in revised manuscript (relevant change in bold):**
"We used tracking data from HURDAT2, which records the storm center’s position
at **four standardized times for weather data collection (synoptic times),**
6:00 am, 12:00 pm, 6:00 pm, and 12:00 am Coordinated Universal Time (UTC)."

\medskip

\begin{shaded}
\textbf{R3 Comment 3:}
Several of the exposures are calculated for the population mean center of each
county. From the reference list, it looks like the authors consistently used
population centers from 2010 throughout the years included in the dataset/R
package. If so, I suggest clarifying this in the text. 
\end{shaded}

**Response:** 

This is correct---the population centers from the US 2010 Decennial Census were 
used throughout. We have added text to the manuscript to clarify this: 

> **Text in revised manuscript (relevant addition in bold):**
"At each 15-min interval, we measured the distance between the storm's
center and each county's population mean center, **as of the 2010 US Decennial
Census** (US Census Bureau 2020)."

\medskip

\begin{shaded}
\textbf{R3 Comment 4:}
In the methods section starting on line 276, the authors describe the flooding
and tornado data from the NOAA storm events database. I'd suggest providing some
information about where the events in this database come from (e.g., that they
are reported events) here in addition to covering it in the discussion.

\textit{Lines 276--281 from original manuscript:}

\begin{quotation}
\noindent
\textit{"To identify flood- and tornado-based tropical cyclone exposures in US
counties, we matched storm tracks with event listings from the National Oceanic
and Atmospheric Administration (NOAA)’s Storm Event Database (NOAA NCEI 2020).
While this database has recorded storm data, particularly tornadoes, since 1950,
its coverage changed substantially in 1996 to cover more types of storm events,
including flood events (NOAA NCEI 2020). We therefore only considered flood
metrics of tropical cyclone exposure for storms in 1996 and later."}
\end{quotation}
\end{shaded}

**Response:** 

As suggested, we have added that these events are based on reports in the
Methods, in addition to our previous discussion of the point in the Discussion:

> **Revised text (relevant addition in bold):**
To identify flood- and tornado-based tropical cyclone exposures in US counties,
we matched storm tracks with event listings from the National Oceanic and
Atmospheric Administration (NOAA)’s Storm Event Database (NOAA NCEI 2020).
**Events are included in this database based on reports from National Weather
Service personnel and other sources.** While this database has recorded storm
data, particularly tornadoes, since 1950, its coverage changed substantially in
1996 to cover more types of storm events, including flood events (NOAA NCEI
2020). We therefore only considered flood metrics of tropical cyclone exposure
for storms in 1996 and later."

\medskip

\begin{shaded}
\textbf{R3 Comment 5:}
Line 294: What is meant by the phrase "traditional tornado event database"?

\textit{Lines 294--295 from original manuscript:}

\begin{quotation}
\noindent
\textit{"The tornado observations from this dataset form a traditional tornado
event database for the US, and so we did not further validate the tornado event
data."}
\end{quotation}
\end{shaded}

**Response:** 

This database is NOAA's official database on tornado events in the United States
[@mccarthy2003nws]. It is the original source for the only other national
tornado database available for the country, the Storm Prediction Center [SPC]'s
National Tornado Database [@center2020storm], which provides a version of the
same data in which tornado events are joined into a single record in cases where
they cross county lines. There is no other comparable source of data on tornado
occurrences that could be used for a comparison as validation.

The NOAA Storm Events database originated as a database for recording tornadoes
in the US in 1953, and was originally called the Climatological Data National
Summary [@Ashley2007]. The database has gone through a number of expansions to
cover more events, and it now includes reports of other types of
hydrometeorological disaster events, including floods, extreme heat and cold,
wildfires, frost, and hail. However, it has maintained its status as the
database of record for tornadoes in the United States.

We have changed the text in the manuscript to make this clearer, changing the
wording to clarify that this is the official NOAA data on tornado events in the
US and that there are no other data sources of comparable scope that could be
used for validation:

> **Text added to the revised manuscript (relevant additions in bold):**
"The tornado observations from this dataset---**along with the derived version
of the data available through the Storm Prediction Center's National Tornado
Database [@center2020storm]---are the official tornado event data** for the US.
**There are no other collections of tornado data comparable in temporal or
geographical scope,** and so we did not further validate the tornado event
data."

\medskip

\begin{shaded}
\textbf{R3 Comment 6:}
Table 1: I suggest providing the rationale for picking the thresholds for
classifying continuous exposures into binary ones within the table. 
\end{shaded}

**Response:** 

Ultimately, this choice will depend on what a researcher considers as plausible
causal pathways for the outcome he or she is studying. For example, if power
outages form a plausible step in the pathway, then the threshold for gale-force
or strong gale-force winds might be reasonable, while if the pathway is more
likely to be through larger-scale building destruction, a higher threshold would
be more reasonable. 

For all outcomes, there are therefore various reasonable choices that can be picked for
the threshold in defining exposure. There is not room to go into these details
within Table 1, so instead we added a separate table to the Supplemental
Material that provides both some reasoning behind the thresholds selected in
this study and also other reasonable thresholds that could be considered.
This is Table S1 of the revised manuscript and has been reproduced as 
Table \@ref(tab:thresholds) at the end of this response document.

We have added text to introduce this supplemental table in the revised manuscript, 
both in the Methods and in the Discussion: 

> **Text in the revised Methods (added text in bold)**:
"For other exposure metrics, each county was
classified as exposed to a tropical cyclone based on whether the exposure
metric exceeded a certain threshold (Table 1). We
picked reasonable thresholds (e.g., the threshold for gale-force wind for wind
exposure; **Table S1**), but others could be used with the open-source data and its
associated software."

> **Text in the revised Discussion (added text in bold)**:
"Finally, to assess patterns and agreement for binary exposure classifications,
we have chosen one set of sensible thresholds for binary classifications based
on continuous metrics (rainfall, maximum sustained surface wind, and distance
from the storm's track), **but other thresholds would be reasonable
depending on hypothesized pathways for a given epidemiological study (Table
S1)**."

\begin{longtable}{lp{35em}}
\caption{Reasons behind the choices of thresholds for binary exposure classifications, as well as discussion of some other reasonable choices. These are provided for the three exposure metrics for which our database includes continuous data, and so a threshold is selected to determine binary exposure based on the metric. This table provides reasoning for the choice of threshold used in this paper as well as guidance on other thresholds that could be considered, depending on the hypothesized pathways for an epidemiological study.} \\
\label{tab:thresholds} \\
\hline
Metric & Threshold choice\\
\hline
\textbf{Distance} & Distance-based exposure was determined based on whether the storm track came within 100 km of county population mean center. This threshold has been used in prior research as a proxy for exposure to hazards from the storm (e.g., Grabich et al. 2015a). Tropical cyclones vary dramatically in size: US tropical cyclones have been observed with radii to maximum winds as small as 20 km and as large as 200 km (Mallin and Corbett 2006; Quiring et al. 2011), and dangerous winds can extend beyond these maximum winds. One study assessed county-level risk and exposure based on a three-tiered definition, with primary counties being those closest to the storm track on either side, secondary counties being adjacent to primary counties, and tertiary counties adjacent to secondary counties, which resulted in an average distance radius of 120 km on either side of the storm track (Czajkowski et al. 2011). Other distance thresholds that could be considered include 60 km and 30 km, both of which have been used in previous research (Grabich et al. 2015a; Grabich et al. 2015b; Currie and Rossin-Slater 2013). However, based on the results presented in the main manuscript, hazard-based metrics should often be used directly rather than a distance-based proxy.\\

\textbf{Rain} & Rain-based exposure was determined based on whether the county had cumulative rainfall of $\ge75$ mm over the period from two days before to one day after the storm’s closest approach and the storm track came within 500 km of the county population mean center. One recent study has highlighted that a two-year rainfall value (which is the median annual maximum rainfall) for a location can provide a useful threshold in identifying rainfall events with potential for societal impacts (Bosma et al. 2020). For some of the more northern, inland communities included in our study area, the two-year rainfall value for two-, three- and four-day windows is in the 65--85 mm range. For example, Pittsburgh, PA, has two-year rainfall values of 69 mm for a two-day window, 74 mm for a three-day window, and 78 mm for a four-day window (US NOAA 2020), consistent with the 75 mm threshold we selected for exposure classification in this paper. However, other thresholds, particularly higher thresholds, would be reasonable in some cases. For example, the two-year rainfall values tend to be much higher in counties of the study area that are further south and close to the coast. The two-year rainfall value for Miami for a three-day window, for example is 180 mm (US NOAA 2020). A variety of definitions have been used previously to identify both extreme or heavy rain (whether associated with a tropical cyclone or not) and tropical cyclone--associated rain. In defining precipitation associated with tropical cyclones, studies have used thresholds of 12.5 mm per day as a metric of regions of ``moderately heavy'' rainfall (Zhou and Matyas 2017) and, as a lower threshold, a lower limit of 10 mm of total storm precipitation---\parfillskip=0pt\tabularnewline

&in conjunction with proximity to the storm's center---in identifying tropical cyclone precipitation events at a location (Feldmann et al. 2019). Other definitions of extreme rain events---including but not limited to tropical cyclone--associated rainfall---are higher than the threshold we use here---for example, one paper defined extreme rain events as cases in which a gauge reported 125 mm or more of rain in 24 hours (Schumacher and Johnson 2006). Studies have also used definitions that are relative to the norms for a given location (e.g., 24 hour rainfall totals over the 50-year return value for the location, which the part of the US east of the Rocky Mountains range from 3.5 in [89 mm] to 13 in [330 mm]) (Schumacher and Johnson 2006; Schumacher and Johnson 2005; Stevenson and Schumacher 2014).\\

\textbf{Wind} & Wind-based exposure was determined based on whether modeled storm-associated peak sustained surface wind was $\ge34$ kts at the county’s population mean center. This threshold is being applied to local winds for each county, and it represents the threshold for gale-force winds on the Beaufort wind scale. This limit is used as the outer limit in measuring storm size through the US National Hurricane Center's wind radii for tropical cyclone forecasts (Cangialosi and Landsea 2016). Other thresholds could be selected based on other points on the Beaufort scale---for example, $\ge48$ kts for capturing storm-force winds or $\ge64$ kts for capturing hurricane-force winds. As a note, hurricane-force winds will be rarely experienced for counties, as it will likely only be observed for very severe storms and even for those, only for counties near the storm's landfall. Many presentations of the Beaufort wind scale include descriptions of the conditions that winds in each category would produce both over land and at sea.\\
\hline
\end{longtable}


\medskip

\begin{shaded}
\textbf{R3 Comment 7:}
Figure 3: The x-axis refers to one of the two wind data sources as "Extended
Best Tracks." Is this the same thing as the wind radii dataset described in the
text? Is so, I suggest clarifying this in the figure label.
\end{shaded}

**Response:** 

Yes, it is the same. We have revised the x-axis on the figure to clarify that
the comparison is based on the wind radii dataset described in the text, as
requested. The original and revised versions of Figure 3 of the manuscript are
reproduced at the end of this response document as Figure \@ref(fig:fig3).

```{r fig3, fig.cap="Reproduction of Figure 3 from the original manuscript (left) and revised version in the resubmitted manuscript (right). In the revised version, the x-axis has been labelled to clarify that this axis is showing results based on the wind radii dataset described in the text, in response to Reviewer 3's Comment 7. The revised version also uses a more focused range for the x axis, as requested in EHP's Request 1.", fig.show = "hold", out.width="50%"}
include_graphics("../figures/windcomparison.pdf")
include_graphics("figures/windcomparison.pdf")
```

\medskip

# Requests from EHP: {-#requests-from-ehp}

\begin{shaded}
\textbf{EHP Request 1:}
Figure 3: Please truncate x-axis scale as you do for Figure S7. Since smallest
value is $\sim75\%$, this would make the data in the relevant range much easier to
appreciate.
\end{shaded}

**Response:** 

We have revised this figure to show an x-axis truncated at the minimum observed
value ($\sim75\%$), as requested. The original and revised versions of this
paper are reproduced at the end of this response document as Figure \@ref(fig:fig3).

\medskip

\begin{shaded}
\textbf{EHP Request 2:}
Main text figures (general)
To ensure that figures are accessible to readers with impaired color vision, do not use color as the sole means of conveying information in a figure unless absolutely necessary. Use different symbols, shading/textures, and line patterns instead of (or in addition to) color to distinguish among different data points. Use contrasting colors that can be easily distinguished from each other when the figure is printed in black and white. For details, see: \url{https://ehp.niehs.nih.gov/authors/figures}.
\end{shaded}

**Response:** 

In all these figures, we have used colormaps that are both accessible to readers
with several forms of color vision deficiency, including the most common type
(deuteranomaly, a type of red-green color vision deficiency), and can be easily
distinguished when the figure is printed in black and white [@viridis;
@van2015mpl]. The colormaps we used are all multi-hue sequential maps that avoid
red-green contrasts, while maximizing dynamic range in comparison to something
single-hued, like grayscale [@nunez2018optimizing; @viridis]. They are all
perceptually uniform across the scale for those with normal color vision
[@liu2018somewhere] and are close to perceptually uniform for those with common
forms of color deficiency [@nunez2018optimizing]. They reproduce their original
sequential gradient when printed in grayscale and so are perceptible when
printed on a black-and-white printer [@van2015mpl; @nunez2018optimizing].
Previous research has found that colormaps in this family allow viewers to more
quickly and accurately judge relative distances in scientific figures
[@liu2018somewhere]. The family of colormaps we used in the figures is currently
considered the gold standard for scientific figures [@nunez2018optimizing].

To help illustrate, in Figures \@ref(fig:fig5check1) and \@ref(fig:fig5check2)
at the end of this response document, we have reproduced the current version of
Figure 5 as it would appear to someone with three common types of color vision
deficiencies: deuteranomaly (defective green cone cells, the most common type),
as well as protanopia (defective red cone cells) and tritanopia (defective blue
cone cells). We have also shown how the Figure would appear in grayscale in
Figure \@ref(fig:fig5check2) (right). We have included similar versions of other
figures from the manuscript in response to later requests from EHP in this
response.

```{r fig5check1, fig.cap="Reproduction of revised version of Figure 5 to check accessibility to readers with two types of color vision deficiency, deuteranopia (left) and protanopia (right).", out.width="90%", fig.align = "center"}
include_graphics("figures/averageexposureonly_check1.pdf")
```

```{r fig5check2, fig.cap="Reproduction of revised version of Figure 5 to check accessibility to readers with a third type of color vision deficiency, tritanopia (left) and for printing the figure in grayscale (right).", out.width="90%", fig.align = "center"}
include_graphics("figures/averageexposureonly_check2.pdf")
```

\medskip

\begin{shaded}
\textbf{EHP Request 3:}
Figure 3: Please check the contrast between the colors used to indicate numbers
of counties. If you can see the gradient when printed in black and white it
should be ok, but some of the differences are very subtle. Truncating the x-axis
might help with this (if possible.)
\end{shaded}

**Response:** 

As we note in our response to EHP Request 2, we used colormaps for all figures
that are designed to be perceptible both for those with common types of color
vision deficiency and when printed in grayscale. We have increased the size of
the circles in Figure 3 of the revised manuscript (reproduced in Figure
\@ref(fig:fig3) at the end of this response) so that the colors are clearer, as
well as truncated the x-axis as requested in EHP Request 1. Also, we show in
Figures \@ref(fig:fig3check1) and \@ref(fig:fig3check2) of this response how the
revised Figure 3 would look with three common types of color vision deficiency
as well as when printed in black and white. The gradient is clear when this
figure is printed in black and white.

```{r fig3check1, fig.cap="Reproduction of revised version of Figure 3 to check accessibility to readers with two types of color vision deficiency, deuteranopia (left) and protanopia (right).", out.width="100%"}
include_graphics("figures/windcomparison_check1.pdf")
```

```{r fig3check2, fig.cap="Reproduction of revised version of Figure 3 to check accessibility to readers with a third type of color vision deficiency, tritanopia (left) and for printing the figure in grayscale (right).", out.width="100%"}
include_graphics("figures/windcomparison_check2.pdf")
```

\medskip

\begin{shaded}
\textbf{EHP Request 4:}
Figure 4: Even with nominally normal color vision and a large high-resolution
monitor, it is difficult for me to make out the color differences in the
symbols---almost all appear black or very dark blue, a dozen or so look yellow,
and a handful look dark grey. Is there any way to increase the contrast in these
colors? Would you consider binning into categories instead of using a
continuous color gradient?
\end{shaded}

**Response:** 

Thank you for these suggestions. 

For the original figure, most of the events resulted in 0% of flood gages
exceeding the flood threshold, especially for events that were not recorded as
floods based on NOAA Storm Events. Therefore, most of the points in the original
were indeed dark blue, representing 0%, but we can see that readers might have
questioned if this was the case or if the scale made it harder to see subtler
differences.

We have made several changes to make this figure easier for readers to see and
interpret. These changes include:

- Binning the values shown by the points fill, so that readers only need to 
be able to distinguish among four colors, rather than across a larger scale.
- Increasing the size of the points, so the fill in each point is easier to 
see. 
- Using a different algorithm to "jitter" the points, so there are no points that
overlap each other. 

We have reproduced both the original and revised versions of Figure 4 in the
manuscript as Figure \@ref(fig:fig4) at the end of this response.

\medskip

\begin{shaded}
\textbf{EHP Request 5:}
Figures 5--6: Please check to see if the colors used can be distinguished if
printed in black and white, and if not, use a “color-blind friendly” palette if
possible. If this is not feasible, we can request an exemption from
accessibility requirements, but we try to do this only when alternatives  would
degrade the information content of  the figure.
\end{shaded}

**Response:** 

As we note in our response to EHP Request 2, we used colormaps for all figures
that are designed to be perceptible both for those with common types of color
vision deficiency and when printed in grayscale Figures \@ref(fig:fig5check1)
and \@ref(fig:fig5check2) at the end of this response show what Figure 5 of the
manuscript would look like under three common types of color vision deficiency
and when printed in black and white. Figures \@ref(fig:fig6check1) and
\@ref(fig:fig6check2) at the end of this response show the same thing for Figure
6 of the manuscript.

For Figure 6 of the manuscript, for some types of color vision deficiency, it
could be unclear which of the two color legends pairs with which part of the
graph. We have therefore added a note to the figure legend to clarify:

> **Figure legend for Figure 6 in revised manuscript (added text shown in bold)**: 
"Agreement between exposure classifications based on different single-hazard
exposure metrics for all storms between~1996 and~2011 for which at least~100
counties were exposed based on at least one metric. Each row shows one storm,
and the color of each cell shows the measured Jaccard index for each pair of
exposure metrics (proportion of counties classified as exposed by both metrics
out of counties classified as exposed by either metric). For Grace in 2003 and
Ida in 2009, there were no county exposures for either the tornado-based metric
or the wind-based metric (indicated by gray squares). Colors to the right of the
heatmap show the number of exposed counties based on any of the metrics, **and
this panel is linked with the color scale labeled '# of counties exposed by
any metric'.** Storms are displayed within clusters that have similar patterns
based on hierarchical clustering."

These figures should therefore not require any exemption from accessibility
requirements, as they should satisfy these requirements.

```{r fig6check1, fig.cap="Reproduction of revised version of Figure 6 to check accessibility to readers with two types of color vision deficiency, deuteranopia (left) and protanopia (right).", fig.show = "hold", out.width="50%"}
include_graphics("figures/jaccard_heatmap_check1.pdf")

include_graphics("figures/jaccard_heatmap_check2.pdf")
```

```{r fig6check2, fig.cap="Reproduction of revised version of Figure 6 to check accessibility to readers with a third type of color vision deficiency, tritanopia (left) and for printing the figure in grayscale (right).", fig.show = "hold", out.width="50%"}
include_graphics("figures/jaccard_heatmap_check3.pdf")
include_graphics("figures/jaccard_heatmap_check4.pdf")
```

\medskip

\begin{shaded}
\textbf{R2 Comment 6:}
Supplemental Material: Please provide your supplemental material file in Word
(.docx) format. For additional information, see:
\url{https://ehp.niehs.nih.gov/authors/supplemental-material}. \end{shaded}

**Response:** 

We have provided the Supplemental Material in the Word file format in our
revised submission.

\medskip

\clearpage

# References for the response {-#references-for-the-response}
