---
title: "Response to reviewers' comments for *\"Assessing United States county-level exposure for research on tropical cyclones and human health\"*"
output: bookdown::pdf_document2
toc: false
bibliography: writing/hurr_exposure.bib
header-includes:
- \usepackage{xcolor}
- \usepackage{framed}
- \usepackage{bm}
- \usepackage{subcaption}
- \usepackage{gensymb}
- \renewcommand\familydefault{\sfdefault}
- \usepackage{sansmath}
- \usepackage{bm}
- \usepackage{soul}
- \sansmath
- \usepackage{booktabs}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{pdflscape}
- \usepackage{float}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{fullpage}
- \usepackage{pdflscape}
- \usepackage{tcolorbox}
- \tcbuselibrary{skins,breakable}
- \usepackage{amsmath}
- \usepackage{xtab}
- \usepackage{rotating}
- \allowdisplaybreaks
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(knitr)
```


# Overall comments {-#overall-comments}

\colorlet{shadecolor}{blue!8}
\begin{shaded}
\textbf{Associate Editor's Overview:}
Developing a consistent and comparable metric for exposure to tropical cyclones
would be a valuable addition to the literature. All reviewers had questions
about the exposure metric, including the challenges of using county-level data
where not everyone in a county would have been exposed and the difficulties of
going from country-level to individual -level risks. Further the comparison of
different storm hazards could be clarified. The reviewers identified several
issues where further explanation would increase the accessibility of the
manuscript.
\end{shaded}

Thank you for the opportunity to revise and resubmit this manuscript. We
appreciate the helpful suggestions from the reviewers for clarifying and
deepening the discussion in the paper.

Broadly, we have: 

1. Added discussion on the question of county-level versus individual-level
data. 
2. Added explanations for how our comparisons among different storm hazards
can help epidemiologists in designing studies and statistical analysis.
3. Added a small analysis of the correlation in rainfall metrics specifically
in cases of high rainfall ($\ge75$ mm of cumulative rainfall in the county
associated with the storm), to deepen the discussion of potential disagreement
in continuous measures of rainfall during extreme conditions. 
4. Changed labeling, axis range, and other presentation details for some
figures, as well as confirmed that the color scales will be accessible.
5. Corrected typos and improved wording throughout based on suggestions of the
reviewers.

# Reviewer 1: {-#reviewer-1:}

## Overall comments {-#overall-comments}
\colorlet{shadecolor}{blue!8}
\begin{shaded}
\textbf{Reviewer 1 Overview:}
I applaud the authors for this work, and their efforts to develop standardized
ways to measure exposure to tropical cyclones. As a disaster scientist, I very
much appreciate efforts that promote our collective ability to learn from and
across disasters to build cumulative science. 
\end{shaded}

**Response:** 

\begin{shaded}
\textbf{Reviewer 1 (R1) Comment 1:}
I very much appreciate the rationale for authors' decision to focus at the
county level, especially given the available health-related data. But, since
tropical cyclones don't necessarily have consistent impacts across an entire
county, I think the limitations of this decision are worthy of further
discussion in the discussion section.
\end{shaded}

**Response:** 

This is an excellent point. We have added text in the Limitations section of the
Discussion on the potential limitations of using aggregated, county-level
exposure data in a research study:

> **Text added to Discussion (added text in bold):** "Second, these data are
aggregated to the county level. **This spatial scale allows for easy integration
with health outcome data aggregated at the county level. Such aggregated or
ecological data is often used for disaster epidemiology, as aggregated data may
be easier to access than individual-level data, especially at a scale that
covers many locations and a long time period and so allows higher statistical
power and a broader range of exposure levels within the study
[@wakefield2008overcoming].**

> "**These ecological exposure estimates, however, set a common exposure level
throughout the county, ignoring within-county variability, even though such
variability often exists. For some hazards, this within-county variation in
exposure level could be stark. For example, tornadoes cause very localized
damage, directly along the tornado's path---a tornado can destroy homes on one
side of a street while leaving those on the opposite side untouched. Levels of
other hazards, like storm-associated winds and rain, will also vary within a
county, but typically with smoother variation. In particular, it will be
unlikely that a county will have one area that is exposed to extremes of these
hazards from a tropical cyclone while other parts of the county are completely
unexposed, as both the wind fields and rain fields of tropical cyclones tend to
be large in comparison to the size of a county.**

> "**Aggregated data can be used to infer contextual level associations---for
example, the association between county-level exposure to a storm hazard and
county-wide rates of a health outcome. However, ecological data is also
sometimes used to infer individual-level associations (e.g., the association
between personal exposure to a storm hazard and personal risk of  experiencing
the health outcome). Individual-level inference from ecological/aggregated data
is susceptible to ecological bias [@greenland1994invited; @portnov2007ecological;
@idrovo2011three]. Researchers who use the data provided here for ecological
studies---with the aim of making individual-level inferences---should be aware
of this potential and could explore approaches for minimizing risk of ecological
bias (e.g., @wakefield2008overcoming).**"

\begin{shaded}
\textbf{R1 Comment 2:}
Are there opportunities to expand this approach internationally? Some decisions
(e.g., county-level) data may restrict cross-country comparisons. Can the
authors discuss this? 
\end{shaded}

**Response:** 

While there would be some limitations in comparisons based on differences in the
scale of county-equivalents, many countries affected by tropical cyclones do
have geopolitical areas that are fairly comparable to US counties, like
municipalities in Mexico, districts in India. There would be a few challenges,
however, in developing the exposure dataset for the hazards that we cover in the
US in the data presented in this paper. We have added text to the paper's
Discussion on opportunities and barriers to expanding this approach
internationally:

> Text added to the Discussion (added text in bold): 
"**This dataset is limited to the contiguous US. Some exposure estimates would
be fairly straightforward to extend internationally. The precipitation data is
extracted and aggregated to the county level from a re-analysis data product.
While the re-analysis product used here (NLDAS-2) only covers the contiguous US,
other similar re-analysis products, as well as other types of precipitation
datasets, have global coverage [@sun2018review]. The wind data are generated
based on a model that is currently US-focused [@stormwindmodel], but could be
extended to other areas, although this would require adding new land/sea masks
within the associated software, as well as accounting for differences across
storm basins in wind averaging periods [@harper2010guidelines] and in the
direction of cyclonic winds in the Northern versus Southern Hemisphere. Further,
since the core of the wind model was developed based on data from Atlantic-basin
storms [@willoughby2006parametric], an extension to other areas should include
separate validation and calibration to ensure it performs appropriately in those
settings. Other wind field modeling software is also available that already
provides a global coverage, like Geoscience Australia's Tropical Cyclone Risk
Model (http://geoscienceaustralia.github.io/tcrm/). For tornado and flood
events, the data described in this paper drew on a US-focused storm events
database, and so international extension of data on these hazards would require
access to similar databases covering other countries. Finally, the relevant
geopolitical boundaries to use for aggregation would vary by country (e.g.,
municipalities in Mexico, districts in India).**"

\begin{shaded}
\textbf{R1 Comment 3:}
In non-health fields, a county's inclusion in a FEMA major disaster declaration
or eligibility for public/individual assistance (all of which are somewhat
reflective of level of damage) is sometimes used as an "exposure" proxy. Have
you considered if/how your exposure assessments relate to inclusion in a FEMA
disaster declaration? In other words, does exposure to any of the hazards align
with the level of damage required for a county to be considered a "disaster
area" and/or worthy of federal assistance by current federal policy? Does this
matter?
\end{shaded}

**Response:** 

This is a great question, and we are aware of a number of studies on the
societal impacts of tropical cyclones that have used FEMA disaster declarations
to assess county-level exposure to tropical cyclones [refs]. 

However, evidence from prior tropical cyclone epidemiology that doing so is
problematic and should be avoided [@grabich2015measuring]. FEMA's disaster
declarations are issued to provide assistance, based on damage assessment, to
individuals or the entire public in certain locations where catastrophes
overwhelm the state or local government [@mccarthy2014fema], and any use of them
for exposure assessment is secondary. Due to the political nature of disaster
declaration process, these declarations are often subject to many political and
economic factors [@mccarthy2014fema; @logue1981research]. One epidemiological
study specifically focused on exposure assessment for tropical cyclone
epidemiology and compared use of FEMA declarations with other methods of
exposure assessment, including one based directly on the storm hazard of wind
[@grabich2015measuring]. They found that exposure assessment based on FEMA
declarations tended to overassign counties to be "exposed," resulting in false
positives [@grabich2015measuring]. A further concern is that, given the
political and economic nature of these declarations, there is likely variation
across time and geography in the likelihood of a given exposure resulting in a
disaster declaration. This is particularly worrisome for a study that seeks to
explore risk across multiple years and affected communities.

We have added some discussion of this point... 

- Future research could expand the work by [@grabich2015measuring] to determine
the extent of these issues with use of FEMA disaster declarations for
epidemiological exposure assessment. This would need to go beyond the comparison
with exposure assessment based on storm hazards, because sometimes the
characteristics of the affected community modifies the amount of disaster that
results from hazards of a certain severity. For example, a community that it
exposed often might be more "hardened" against damage from the storm (any work
from Seth on this) and so might experience less infrastructure damage from a
tropical cyclone with certain characteristics compared to a community that is
rarely exposed to tropical cyclones. One interesting direction would be to
investigate if there is evidence of differences in probablity of disaster
declarations at geopolitical boundaries, like when comparing counties on either
side of a state border.

There are some cases where damage reports are directly used to infer the
intensity of the weather event. For example, damage surveying is used following
a tornado to assign the tornado's class on the [enhanced] Fujita scale. However,
this is not the intent of FEMA damage reports, which instead are used to
determine post-disaster aid [?] and are influenced by a number of political
factors.

> "Because of the historic lack of direct measurements and remotely sensed
tornado wind speeds at or near group level, damage surveying has remained the
most common form for indicating tornado strength. ... The occurrence of a direct
tornado strict upon a fixed, sufficiently sturdy, and well-calibrated wind
measuring station is quite rare. Only 31 direct in situ tornado observations are
evident between 1894 and 2011." [@edwards2013tornado]

This question could be explored to some degree through a study that uses the 
physical hazard data provided in this dataset as the exposure and FEMA disaster
declarations as the outcome. Such a study could clarify the degree to which 
each hazard---and its intensity---helps in explaining that probablity that 
a disaster declaration is issued. It could also clarify the degree to which 
other factors, beyond the intensity of these hazards, drive the probability of
a county receiving a disaster declaration. For example, if two counties 
are exposed to a storm-associated hazard of the same intensity, they may 
still have different probabilities of associated societal impacts, as a 
community that is regularly exposed to that hazard may be more "hardened" 
against it. Other differences in infrastructure and factors associated with 
vulnerability could similarly introduce variability. 

At a more complex level, studies could explore the causal pathways from the 
initial physical hazard to resulting damage/property loss through to associations
with health outcomes, to provide a clearer idea of the direct and indirect 
pathways leading to health impacts from tropical cyclones.

For both of these studies, however, there may be better sources of damage
estimates to use than FEMA disaster declarations. FEMA disaster declarations 
can be prone to political and other influences, and since their primary function
is not to record damage but rather to be a step in getting help to communities, 
they can be problematic to use as an unbiased estimate of damage levels in 
epidmiological research. One study, for example, found ... . There are other
estimates of property damage that might be less problematic, including the 
estimates recorded in NOAA's Storm Events database and data from private 
insurers, although the quality of disaster loss data is well known to be
less than ideal for secondary research [can be prone to bias through a number
of mechanisms] [ref to Gall paper?].

FEMA declaration is largely subjective, requiring state governors and/or tribal
chiefs to submit a proposal for government aid on the basis that local resources
are overwhelmed [@FEMA]. As a result, using FEMA disaster declarations as
exposure classification to hurricanes restricts exposure to 1) those states
whose resources are overwhelmed and 2) those states who apply for FEMA aid.
(FEMA is likely less objective given societal effects (politics) [@Kunkel1999])
As a result, other severely impacted counties and states that may not fit FEMA
criteria are excluded and may therefore be incorrectly identified as unexposed,
preventing researchers from assessing the full impact of a given hurricane
[JMF-- the Grabich 2016 study actually found that FEMA assigned the highest
exposure, and may overestimate the health effects of a hurricane; GBA-- let's
think about why that might have had "false positives". I think that would be an
interesting point to include here.]. Researchers have found heterogeneity in
exposure assignments of counties, both within and between storms, when different
metrics of exposure are used [@Grabich2016; @Grabich2015]. Such
missclassification can have important consequences in assessments of the public
health and economic impacts of tropical storms.

One study used FEMA disaster declarations as a binary indicator of tropical
storm exposure [@Grabich2016]. They found that this metric tended to assess a
lot more counties as "exposed" to a storm than distance- or wind-based metrics
[@Grabich2016], although they only compared the two exposure metrics as applied
to four storms (Hurricanes Charley, Frances, Ivan, and Jeanne in Florida in
2004).

Data on storm impacts, including property damage or insurance claims data, are
sometimes available through storm databases or publications (e.g., NOAA's Storm
Events data, *Monthly Weather Review*). While such data could potentially be
used to create an exposure metric, tropical storm damage data requires some
normalization, to incorporate both changes in dollar value over time and also
changes in development in at-risk areas, to be comparable over extended time
periods [@Pielke1998].

\begin{shaded}
\textbf{R1 Comment 4:}
Line 95---wondering if there is a better word for "hits" here. Maybe "exposed?"

\textit{Lines 92--95 in the original manuscript:}

\begin{quotation}
\noindent
\textit{"In many cases, these studies analyze multi-year, multi-community data,
allowing them to estimate average associations over many disasters and to
explore how a disaster's characteristics, or the characteristics of the
community it hits, modify associated health risks (e.g., Anderson and Bell 2010;
Son et al. 2012; Liu et al. 2017)."}
\end{quotation}
\end{shaded}

**Response:** 

Thank you for this suggestion. We have changed the wording in this sentence: 

> **Text in revised manuscript (relevant change in bold):**
"In many cases, these studies analyze multi-year, multi-community data, allowing
them to estimate average associations over many disasters and to explore how a
disaster's characteristics, or the characteristics ofthe **affected
communities**, modify associated health risks (e.g., Anderson and Bell 2010; Son
et al. 2012; Liu et al. 2017)."

We also used the word "Hits per county per decade" in the legend for Figure 5 in
the main text and Figure S3 of the Supplemental Material. We have revised the
wording in these figures to "Exposures per county per decade".

```{r fig5, fig.cap="Reproduction of Figure 5 from the original manuscript (left) and revised version in the resubmitted manuscript (right). In the revised version, the figure legend was changed from '\\textbf{Hits} per county per decade' to '\\textbf{Exposures} per county per decade' (bold used to highlight change). Figure caption in the revised manuscript: \\textit{Figure caption.}", fig.show = "hold", out.width="40%", fig.align = "center"}
include_graphics("../figures/averageexposureonly.pdf")
include_graphics("figures/averageexposureonly.pdf")
```

\begin{shaded}
\textbf{R1 Comment 5:}
Lines 137--139, suggest breaking into two sentences

\textit{Lines 137--141 from original manuscript:}

\begin{quotation}
\noindent
\textit{"While other approaches have been developed to incorporate storm
hazards, particularly wind, into exposure assessment (e.g., Grabich et al.
2015a; Zandbergen 2009; Czajkowski et al. 2011), there is not yet a standard
approach, and when different studies use different datasets or storm hazards
when assessing storm exposure, it becomes difficult to compare and aggregate
findings."}
\end{quotation}
\end{shaded}

**Response:** 

This is a great suggestion. We have split the cited sentence into two sentences:

> **Text in revised manuscript:**
"While other approaches have been developed to incorporate storm
hazards, particularly wind, into exposure assessment (e.g., Grabich et al.
2015a; Zandbergen 2009; Czajkowski et al. 2011), there is not yet a standard
approach. When different studies use different datasets or storm hazards
when assessing storm exposure, it becomes difficult to compare and aggregate
findings."

\begin{shaded}
\textbf{R1 Comment 6:}
This is not a comment on the paper, but the authors may also want to consider
publishing their work in DesignSafe, which is "the web-based research platform
of the NHERI  [NSF-Funded Natural Hazards Engineering Research Infrastructure]
Network that provides the computational tools needed to manage, analyze, and
understand critical data for natural hazards research." It is the data
repository widely used by interdisciplinary hazards and disaster researchers.
\end{shaded}

**Response:** 

It turns out that, because we have shared the data as an R package, it should
already be accessible to use on the DesignSafe platform.

DesignSafe is an online platform sponsered by the National Science Foundation as
part of the Natural Hazards Engineering Research Infrastructure
[@rathje2017designsafe]. This platform can be used to store, share, and publish
citable data related to natural hazards through its Data Depot. It goes beyond a
data repository, however, and also allows researchers to conduct data
integration and analysis in the cloud through its Discovery Workspace, while
drawing on data stored in the Data Depot and while using scientific programming
tools.

As part of its platform, DesignSafe allows users to work in a number of
programming languages, including R (through Jupyter Notebooks)
[@rathje2017designsafe]. Therefore, DesignSafe users already have access to any
data shared through an R package, including the data we present in this
manuscript, without the data needing to be separately loaded and published
through DesignSafe's Data Depot.

This is great, because it might otherwise be difficult to try to maintain copies
of the dataset in different repositories (i.e., one copy as an R package and
another as a DesignSafe dataset), and there could be confusion about things like
the equivalence of the two copies. R packages are citable through a standard
format that gives the R package version, while data publicly shared on
DesignSafe is assigned a digital object identifiers (DOI)
[@rathje2017designsafe], and so different citations would be used for the same
dataset depending on how the user accessed it. If updates of one copy were not
immediately updated for the other copy, the copies could get out of sync, and
maintaining two synced copies would add would add an extra layer of work with
each update to the R package (we have been updating several hazards as new data
becomes available). DesignSafe's interface with R avoids this problem by
allowing its users direct access to our package and its data through that path,
as it allows for a single, citable, licensed version of the data stored and
shared from a single location, with no question of syncing separate versions of
the data or clarifying licensing and citation practices across two storage
locations.

We have added a note in the manuscript to let readers know that, because the
data is shared through an R package, and because DesignSafe's Discovery
Workspace allows users to work in R, the data can be fully accessed and used for
those working on the DesignSafe platform by using the same code in a DesignSafe
Jupyter notebook that would otherwise be used on a local computer.

> **Text added to the Methods of the revised manuscript (added text in bold):**
"**By sharing this data as an R package, it is also accessible through 
cloud-based computing platforms that incorporate Jupyter notebooks, including
the National Science Foundations's DesignSafe platform for natural hazards
engineering research [@rathje2017designsafe].**"

# Reviewer 2: {-#reviewer-2}

\begin{shaded}
\textbf{Reviewer 2 overview:}
Exposure assessment is essential to study health impacts from disasters, but it
is also a great source of biases due to a lack of a good and fine measure of
exposures during the chaos. Often, epidemiologists use existing data at large
population level such as county level weather data which provide no individual
level exposures but give average exposure at the population level. So it is very
challenging to draw a good causal inference to health outcomes that measured at
the individual level. This manuscript is a well-written report with generously
shared data and programs on US tropical cyclones. This will contribute
significantly to disaster epidemiology which is still a new area and suffering
from good exposure ascertainment.

While I enjoyed reading the manuscript very much, I have one question I could
not figure out clearly.

One objective is to investigate patterns and agreement between different
exposure measures. I do not clearly understand the value of studying agreement
between exposures, while each exposure should be measured separately, so both
individual and overall effects can be assessed.  I do understand using "distance
from the storm" as exposure is only proxy measure and highly sensitive to
misclassification, but because direct storm hazards such as rainfall, flood,
storm surge, and tornado are publicly accessible as authors indicated, authors
could just simply guide readers to use the best exposure data rather than
conducting a quite intense analysis. This may be due to my limited knowledge, so
it would be appreciated if the authors provide explicit explanations on the
purpose and benefit of the analysis.

\end{shaded}

Thank you for this suggestion, and we can see that we could have more clearly 
articulated in the original manuscript how our assessment of patterns in and 
among exposures to specific hazards can help epidemiologists to design their
studies and the statistical methods to analyze the resulting data. 

1. If exposure to the different hazards strongly agrees (i.e., when a county is
exposed to one hazard, they are typically also exposed to a second), this would
have several implications for epidemiological research, especially for
multi-year studies:
  + From a statistical modeling point of view, if you tried to include each
  hazard as a separate independent variable, you would run into problems with
  collinearity if fitting the study data with a generalized linear model or
  other regression model. One implication of this is that the confidence
  intervals on effect estimates for the two variables would be very inflated,
  because the model would struggle to figure out which of the two variables the
  weight should be placed on, and so this uncertainty would be reflected in the
  size of the associated confidence intervals.
  + If you see an effect, it would be very hard to try to determine if the
  causal pathway goes through the first hazard or the second. To mitigate
  effects, it is important to understand the pathway through which they happen,
  and it might be easier to get clues towards pathways if the hazards are better
  separated. (Analogy: if you were able to conduct a controlled experiment to
  look at the effects of each hazard, jointly and in combination, you would
  design separation in the assignment of these treatments, so you could
  distinguish. We can't do that because we are limited to an observational
  analysis, but the area of causal inference is doing a lot to explore how we
  can design observational studies and analyze data to come closer to what we
  might be able to see with a randomized control study, and which of those
  methods we might be able to use for tropical cyclone epidemiology will depend
  on the interplay of these patterns of exposures.)
2. These results will not be surprising to an atmospheric scientist / disaster
researcher, but there are environmental epidemiologists who have done research
on other exposures and are now starting to study tropical cyclones (session at
ISEE, for example). It is useful for epidemiologists entering into tropical
cyclone research to understand these facets of tropical cyclone exposure,
including the distinction between the very coastal wind exposures and other
hazards that tend to reach further inland.
3. There are epidemiological studies that have used only one hazard for exposure
assessment. This has most often been the case for wind. If this exposure
assessment is meant to capture "tropical cyclone exposure" in general, with all
the hazards that are related to tropical cyclones, then there is the chance for
exposure misclassification if this single hazard is used.
4. In the past, it was a very complex issue to try to pull together data on all
these hazards. We agree that now that we have provided a dataset with all of
them, epidemiologists should be thinking about multi-hazard analyses. However,
there still will be some barriers to that. [Recent work on multi-pollutant
exposures and the complexities associated with that.] If one of the hazards
dominates in explaining associated health risk, then in some cases it might be
reasonable to focus on that hazard, but in that case it is critical, based on
our results, to remember that the exposure assessment is not capturing all
hazards of the storms.

We have moved other suggestions that you sent in "Track Changes" of the Word 
document for the original manuscript and address each in detail below.

The largest-scale county-level exposure study is likely that of @Zandbergen2009,
which estimated exposure in US counties to all US landfalling Atlantic basin
tropical storms between 1851 and 2003, using both distance and an exposure
metric that incorporated distance and windspeed. They used this exposure
evaluation to create maps of total exposure to tropical storms within US
counties, as well as to explore associations between a county's long-term
exposure to tropical storms and its location, distance from the coast, size, and
shape [@Zandbergen2009]. @Kruk2010 explored exposure to hurricane-related winds
in the United States, including inland areas, for 1900-2008.

Geographical patterns can differ for these different
storm hazards, and so exposure assessment based on one hazard could create
exposure misclassification if the pathway for health risks is through a
different storm hazard.   

A tropical cyclone's high winds can bring health risks and property damage
through structural damage of houses and other buildings, falling trees, and
wind-borne debris [@rappaport2000] and cause power
outages [@liu2005; han2009], which introduce a number of threats to
human and ecological health, including water quality risks if the outage affects
wastewater treatment plants [@mallin2006].  Other risks can exist
without severe wind; for example, one study found that most of the direct
hurricane-related deaths in the US between 1970 and 1999 occurred in cases
when wind was below hurricane strength, including for Tropical Storms Charley
in 1998 and Alberto in 1994 [@rappaport2000].  Tropical cyclones can
produce excessive rain, especially in certain topographies (e.g., near
mountains), so counties well inland sometimes experience more extreme rain than
coastal counties. Flood risks from tropical cyclones can result from this rain,
although the two risks are not perfectly correlated [@chen2015]. In the
US, over half of hurricane-related direct deaths from 1970 to 1999 from
Atlantic basin storms were a result of freshwater
flooding [@rappaport2000]. Flooding can also degrade water
quality [@mallin2006], which can threaten both human and ecological
health.

If a study misclassifies exposure to the hazard or hazards that cause the
health risk being studied, the study will generate biased estimates of tropical
cyclone risks and impacts. Further, when different studies use different
methods to assess exposure to tropical cyclones, their results are difficult to
meaningfully compare and aggregate. If different methods identify
similar sets of communities as ``exposed'',  these concerns are less serious.
However, if different methods differ substantially in which communities they
identify as ``exposed'', it makes it very important that epidemiological
studies are thoughtful in how they assess exposure.

These analyses can also help researchers compare and interpret results from
previous epidemiological studies that have assessed tropical cyclone exposure in
different ways, helping to [synthesize] previous epidemiologic results to form a
better picture of how these storms can affect human health.

However, these frequency maps, together with evidence from
specific tropical cyclones (Figures \ref{fig:ivanexposure}
and \ref{fig:jaccard}),  do illustrate the potential for strong differences in
spatial patterns in tropical cyclone exposures, depending on which tropical
cyclone hazards are considered.  A few previous studies have sought to
determine county-level exposure to tropical cyclones over multi-year periods,
including @zandbergen2009, which estimated exposure in US
counties to all US landfalling Atlantic-basin tropical cyclones
between 1851 and 2003, using both a distance-based metric and a metric that
combined distance and windspeed, and [@kruk2010], which explored
exposure to hurricane-related winds in the US, including inland areas,
for 1900--2008.  Our results suggest that such exposure assessments may
perform well in capturing some tropical cyclone hazards (e.g., wind), but
likely miss other potentially dangerous tropical cyclone exposures, especially
for hazards that repeatedly threaten northern or inland counties (e.g., rain,
flooding).

Therefore, use of distance to assess tropical cyclone exposure for impact
studies could result in problematic exposure misclassification, which could mask
true associations, even strong associations, between tropical cyclone exposure
and outcomes of interest in impact studies [@savitz2016interpreting;
armstrong1998effect]. The use of a single hazard-based metric (e.g., wind) could
cause similar problems if the impact is driven, at least in part, by a different
hazard or by multiple hazards of the tropical cyclone.

If measurements of one storm hazard are used as a proxy to capture exposure to
a different hazard, or a non-hazard proxy like distance to the storm track is
used to assess exposure to storm hazards, the resulting exposure
misclassification could be differential (i.e., associated with the outcome of
interest or with factors associated with risk of the outcome of interest). 

Therefore for most tropical cyclones, since exposure assessments often differ
across storm hazards, as well as between each storm hazard and a distance-based
proxy measurement, exposure misclassification is a potential risk.

Further, we investigated agreement in exposure assessment across these metrics
when applied individually. We found large differences in which counties are
exposed to different hazards of tropical cyclones and that distance is, at
best, a moderate, and often a very poor, surrogate for exposure to the specific
tropical cyclone hazards of high wind, extreme rainfall, flooding, and
tornadoes. Use of distance as a surrogate for any of these hazards, or exposure
assessment based on one hazard when the pathway for health impacts is in part
or full through another storm hazard, could lead to exposure misclassification.
In the case of tropical cyclone risk and impact studies, including
epidemiological studies, this would result in biased estimates.  Our findings
highlight the importance of clarifying the potential pathway from tropical
cyclone hazards to health impacts when conducting tropical cyclone
epidemiological studies, and then basing exposure assessment on these hazards.


**Response:** 

\begin{shaded}
\textbf{Reviewer 2 (R2) Comment 1:}
Line 164: Isn't it obvious to use better exposure measures than proxies? Wind,
rainfall, tornado, and flood are direct products of storms while distance is
only proxy of those exposures. Rather than calculating agreement among the
exposures, it would be important to note that these are independent hazards that
can come together or come with different intensity and at different time
windows.

\textit{Lines 164--166 from original manuscript:}

\begin{quotation}
\noindent
\textit{"They found important differences, concluding that a study may be prone
to bias from exposure misclassification if distance to the storm track is used
as a proxy for exposure (Grabich et al. 2015a)."}
\end{quotation}
\end{shaded}

**Response:** 

It is obvious that it is typically better, but in cases where the proxy is much
more practical to measure, it may need to be more than just a bit better, but
instead substantially better. Also, there could be some cases where the proxy
might actually be better---for example, if the proxy can be measured with very
low level, while the direct exposure measure might only be measurable with high
error. In that case, the proxy could be the better metric and might provide a
more precise estimate of the association with health risk.

[Env. Epi. book---advantages and disadvantages of different types of measures]

We agree that it is helpful to clarify for readers that these independent
hazards can come together in a storm event at different intensities and at
different time windows. We have added some text on this:

[Added text]

However, we disagree that this should be down as an alternative to calculating
agreement. Epidemiologists need to understand how these exposure measurements
agree to help in planning studies and statistical approaches to interpreting the
resulting data. In this sense, "agreement" is providing a measure similar to
correlation for continuous variables; our choice of agreement with the binary
exposure classifications reflects the more common use of binary classifications
for most tropical cyclone epidemiology to date.

Epidemiological studies might focus on tropical cyclone exposures at two levels.
First, they might wish to focus on a specific hazard. For example, a study might
hypothesize that mold growth following flooding from a tropical storm might
increase risk of respiratory health outcomes in the months after a storm. In
this case, the clear causal pathway recommends a focus on one of the storm
hazards, rather than all storm hazards. However, other storm hazards could
confound the association between flood exposure and respiratory health risk if
the other hazards are ignored *and* if they are associated with both the
exposure of interest (storm-associated flooding) and the health outcome. For
example, if wind exposure assessments are in high agreement with flood exposure
assessments for storms, and if wind affects risk of respiratory outcomes by
causing extensive property damage and creating extreme psychological stress,
which can have repercussions on respiratory health, then the wind hazard could
confound the estimated association between storm-related flood exposure and the
health outcome.

This bias would be higher when the confounder and exposure of interest are more
strongly correlated (for continuous variables) or in higher agreement (for
categorical variables). Conversely, if the exposure and the potential confounder
are, in practice, only weakly associated, then the potential for confounding is
low. The Jaccard index is one measure of similarities between categorical
(including binary) coefficients.

Another consideration is collinearity. If one hazard is the exposure of interest
and another hazard is a potential confounder, then the model should adjust the
confounder or it will be misspecified and be prone to bias. However, if the
exposure of interest and the potential confounder are very strongly
correlated/in agreement, then including both as independent variables in a
regression model often causes numerical problems in fitting the regression
model, including instability in the estimated coefficients and inflated
variance. In very extreme cases, the statistical software may not be able to
invert the matrix [? only for linear regression?].

Therefore, it is helpful for epidemiologists to understand if these independent
storm-associated hazards tend to be strongly associated (in high agreement),
both within specific storms (for single-storm studies) and on average across
most large storms (for multi-storm studies). This knowledge will help them in
identifying potential confounders if they are interested in studying the effects
of a single hazard of the storm and also in determining if control for these
potential confounders among other storm hazards might cause numerical problems
or other collinearity-associated problems when fitting a regression model.

If a researcher wishes to study the storm as a whole, then the researcher 
could either seek a single measure to capture exposure to the storm as a whole
or investigate more complex models that model the influence of the storm through
pathways of several different hazards. 

If a researcher wants a single measure for the storm as a whole, then it is
helpful to know how well exposure assessments based on the hazard components of
the storm agree with each other. If there is strong agreement, then any of those
hazards could be used as a primary marker of exposure assessment. This is
conceptually similar to checking for observer / rater agreement.

> "When the row and column variable srepresent different observers' rating the
same subjects or objects, interest is focused on *observer agreement* rather
than mere association. In this case, measures and tests of agreement provide a
method of assessing the reliability of a subjective classification or assessment
procedure." [@friendly2015discrete]

> "In assessing the strength of *agreement* we usually have a more stringent
criterion than in measuring the strength of *association*, because observers
ratings can be strongly associated without strong agreement. For example, one
rater could use a more stringent criterion and thus consistently rate subjects
one category lower (on an ordinal scale) than another rater. More generally,
measures of agreement must take account of the marginal frequencies with which
two raters use the categories. If observers tend to use the categories with
different frequency, this will affect measures of agreement."
[@friendly2015discrete]

When you have an array of two or more exposures that are strongly correlated,
including them together in a regression model can cause problems from
collinearity [@schisterman2017collinearity].

> "Environmental exposures ... often consist of an array of related individual
factors that may be highly correlated with each other, causing concern about the
impact of collinearity when attempting to identify individual effects."
[@schisterman2017collinearity]

**Commonly have multiple factors of interest, correlated**

In some areas of epidemiology, researchers have moved toward the idea of a 
"web of causality": 

> "'Multiple causation' is the canon of contemporary epidemiology, and its
metaphor and model is the 'web of causation'. Expressed through the notion
of 'multifactorial etiology' and embedded in the statistical techniques
of 'multivariate analysis', the belief that population patterns of health 
and disease can be explained by a complex web of numerous interconnected 
risk and protective factors has become one of this discipline's central 
concepts. Equally entrenched is the corrolary that epidemiology's power
to improve the public's health rests upon its ability to identify---and predict
the results of breaking---selected strands of this causal web." [@krieger1994epidemiology]

This is in contrast to focusing on single factors: 

> "Expressly challenging the still-pervasive tendency of epidemiologists to
think in terms of single 'agents' causing discrete diseases, the provocative
metaphor and model of the 'web' invited epidemiologists to embrace a more
sophisticated view of causality." [@krieger1994epidemiology]

In this framework, epidemiology helps by identifying where it would help to
break strands of the web: 

> "Using this model, MacMahon et al. drew several important inferences about
prevention and research that remain part of epidemiologic thinking to this day.
Arguing that 'to effect preventative measures, it is not necessary to understand
causal mechanisms in their entirity', they stated that 'even knowledge of one
small component may allow some degree of prevention' since 'wherever the chain
is broken the disease will be prevented'." [@krieger1994epidemiology]

In other words, if you can identify *necessary* causes, you can craft 
*practical* interventions. 

In some fields of epidemiology (e.g., nutritional epidemiology), it is common 
to study cases where there are multiple potential risk factors, and these 
factors are often strongly correlated [@schisterman2017collinearity].

> "The dynamics of many important ecological and evolutionary processes are 
often influenced by multiple interacting factors, and attempting to understand 
the dynamics of such systems presents difficult technical and philosophical 
problems." [@petraitis1996inferring]

> "Often in practice, investigators are faced with decisions of how to handle
highly correlated variables." [@schisterman2017collinearity]

**Methods to tackle multiple causality**

When the aim is to explore the influences of multiple associated factors, there
are some different strategies that can be used. These include multiple
regression and path analysis:

> "Evolutionary ecologists have recently turned to path analysis, multiple
regression, and related techniques to analyse systems of multiple causality.
While these techniques are often seen as competitors, they are, in fact,
related, and as a result the problems of these techniques are similar."
[@petraitis1996inferring]

**Implications of collinearity**

> "Two variables are defined as collinear if one can be expressed as an exact
or near linear combination of the other. ... In general, collinearity between
variables is described by the magnitude of their correlation." 
[@schisterman2017collinearity]

It can complicate inference of model coefficients. This is always a concern for 
inferential models. For predictive models, it can be a problem if you later use
the model to predict in a case where the correlation structure is different than
in the training data. 

> "The dependence of regression and path coefficients on the correlation
structure is often glossed over. Yet this dependence means that inferences about
the strengths of path coefficients are conditional upon the correlational
structure among the predictor variables. If the correlation structure of the
sample does not match the correlational structure of the population then
regression and path coefficients will not estimate relative strengths
accurately. This can occur if sampling is not random or if analysis is based on
data from experiments with factorial designs, which break the correlational
structure amnog the predictor variables." [@petraitis1996inferring]

These implications can depend on whether the modeling goal is predictive or 
inferential. 

> "When the goal is to choose covariates based on their predictive value, 
concerns about collinearity stem from the desire to improve parameter estimates'
precision, which can be inflated when highly correlated covariates are included
in the model." [@schisterman2017collinearity]

> "In a causal framework, on the other hand, interest is often centered on a
specific effect from a correctly specified causal model, even in the presence of
highly correlated data, despite a potential loss of statistical efficiency.
Under such a viewpoint, bias due to misspecification of the causal associations
is a more devastating result than reduced precision of parameter estimates
correctly estimated. Furthermore, a precisely but incorrectly estimated
parameter may lead to invalid inferences. ... Relying on conventional
variable-selection algorithms and standard analyses can ignore important
confounders and even produce extremely imprecise confidence intervals."
[@schisterman2017collinearity]

The amount of bias in the unadjusted model (in terms of bias from an unadjusted
confounder) depends on the strength of association between the exposure of
interest and the potential confounder [@schisterman2017collinearity].

> "This is especially important for the confounding variable scenario, where 
correctly specifying the model may result in adjusting for a nearly 
collinear variable." [@schisterman2017collinearity]

The degree of agreement will influence the degree of confounding in an 
unadjusted model, for either one or two confounders: 

> "In this confounding scenario, increasing correlation exacerbates the bias
due to misspecification, similar to results for a single confounder."
[@schisterman2017collinearity]

Extreme collinearity can create numerical problems
[@schisterman2017collinearity].

> "Under the extremely unlikely scenario of extreme collinearity [(abs. rho of
0.999 or higher)], the numerical matrix inversion required for estimation fails,
resulting in unstable and biased predictors." [@schisterman2017collinearity]

> "Only for extreme values of rho cloes to 1 or -1 is beta-squareed
nonidentifiable due to numerical instability; otherwise, the correctly specified
model produces the most desirable (e.g., unbiased) estimates of the total effect
of the exposure on the outcome." [@schisterman2017collinearity]

Collinearity can cause imprecision in coefficient estimates, even if those
estimates are unbiased [@schisterman2017collinearity].

> "Multicollinearity: A term that refers to correlation among the independent
variables in a multiple regression model; it is usually invoked when some 
correlations are 'large', but an actual magnitude test is not well defined.[1]"
[@gunasekara2008glossary]

> "Collinearity occurs in multiple regression models when two (or
more---multicollinearity) exposure variables are included in the model but are
so similar to each other that they are essentially measuring at least part of
the same thing. Collinearity creates problems with interpreting the analysis by
affecting the standard errors of the variables (as it increases variance)[24]
and by causing biased estimates for one or both of the collinear terms
(sometimes dramatically so) [25]. If there is perfect collinearity between two
variables then one should be dropped from the model. However, dropping exposure
variables to reduce multicollinearity may lead to bias in the model from
dropping useful information." [@gunasekara2008glossary]

> "It might be argued in some cases that it is appropriate to include
(collinear) variables in a model even when they are highly correlated, because
the theoretical basis for including the variables is strong and the results of
the model are consistent with expectations [10]. In such cases, models with and
without the variables would be tested and compared. Such consideraion of
alternative models tends to be described as an issue of (mis)-specification in
econometrics. However, in epidemiology this tends to be described in terms of
mediating and *intermediary variables*. Thus, an epidemiologist might argue for
including an intermediate variable in a model (and testing it against a model
without the variable) because including that variable might elucidate direct and
indirect pathways between the exposure and outcome variables. An econometrician
might do the same thing but describe this in terms of finding the correct model
specification." [@gunasekara2008glossary]

**Techniques to address collinearity**

There are study designs that might help to separate different effects. For
example, some matching designs might help in creating a subset of data in which
the association that exists in the full dataset is broken (e.g.,
@leal2012multicollinearity).

> "Matching is typically employed to reduce model dependence and estimate 
associations in a more empirical way than would be necessary without 
matching. In line with this practice, matching was employed as a diagnostic
tool to verify whether the effects of environmental variables that are highly 
correlated with each other could be disentangled." [@leal2012multicollinearity]

You could also explore techniques of dimension reduction, to create a smaller
subset of independent variables for which the correlation is broken. For
example, you could use PCA [@leal2012multicollinearity].

There are other techniques that allow some bias in estimates in an attempt to
reduce problems from collinearity, like ridge regression
[@petraitis1996inferring].

> "Several biased estimation techniquest ahve been proposed as a way to combat
collinearity (Myers 1990). These are controversial methods. Ridge regression,
for example, biases the regression coefficients in order to improve the model's
ability to predict the criterion variable. The estimates of the individual
regression coefficients may be very biased even though when taken together they
provide a very good prediction of the criterion variable."
[@petraitis1996inferring]

> "When correct specification of the models still leads to 
problems for conventional regression methods, one can apply modern techniques
such as Bayesian hierarchical models with shrinkage estimators."
[@schisterman2017collinearity]

**Are effects of different factors 'separable'?**

Epidemiology often aims to inform policy decisions. In this case, it can be
important to characterize the separate effects of different associated factors
that contribute to the health risk [@leal2012multicollinearity].

> "To develop efficient public health interventions addressing the obesity 
epidemic, it is important to identify exactly which aspects of the
environment influence obesity risk. For example, demonstrating that the density 
of fast-food restaurants was associated with obesity risk and demnstrating that
the density of sports facilities was associated with obesity risk would lead to 
different interventions." [@leal2012multicollinearity]

However, it can be very difficult to separate the effects of different factors
if those factors are strongly correlated [@leal2012multicollinearity]: 

> "However, many neighborhood characteristics, especially those related to the
densities of physical features and services, are strongly correlated with each
other. Therefore, it is not clear whether it is even possible to disentangle the
effects of these different environmental dimensions, even if they are
hypothesized to influence obesity risk through distinct causal pathways."
[@leal2012multicollinearity]



\begin{shaded}
\textbf{R2 Comment 2:}
Line 354: I am not sure why different exposures should be compared and measured
the agreement. I would just treat each exposure as single independent exposure
and give a summary score by summing all the exposure with some weighting for
multiple exposures.

I may not understand the importance of this approach. If so, adding the
explanation would help readers not to questioning.

\textit{Line 354 from original manuscript:}

\begin{quotation}
\noindent
\textit{"Finally, we investigated how well exposure assessment agreed across
these metrics."}
\end{quotation}
\end{shaded}

**Response:** 

As we note in our general response to your comments, we agree in general with
your instinct here. However, there are some subtle points to note.

First, we understand the potential appeal of creating a single exposure summary
score based on a summation of exposures. This simplifies some of the statistical
modeling, for example, and also provides a single exposure estimate to provide
as the key study conclusion. However, in many areas of tropical cyclone
epidemiology, it is unlikely that exposure to each type of hazard should be
given equal weight. For example, an analysis of respiratory health in the months
after the storm might hypothesize a strong connection with flooding, and smaller
connections with rainfall and winds (through the pathway of property damage).
Use of a pre-defined summation of exposure to each hazard would mask some of the
potential association.

While a summary statistic could be defined that adds a weight to each exposure,
it would often be unclear *a priori* what weight each hazard exposure should be
given. A better method might be to use a regression framework, where each hazard
exposure is included as a separate explanatory variable, and the model uses the
observed data to determine the appropriate weight for each hazard in terms of
modifying risk to the outcome of interest. To explore the influence of multiple
concurrent hazards, interaction terms could be added.

However, if our analysis had found that hazard exposures follow very similar
patterns, in terms of the counties exposed during a storm, then this strong
agreement would result in a number of problems in fitting and interpreting these
types of models. First, strong agreement in two or more of the explanatory
variables in a regression model leads to collinearity, which among other
implications often leads to a large inflation of confidence intervals for the
effect estimates of the correlated explanatory variables. Further, if the hazard
exposures were in strong agreement, then it would be difficult or impossible to
disentagle the effects of the separate hazards, as a strong effect for one would
be impossible to interpret with confidence, since the strong agreement in the
data (and resulting low co-variability in the variables in the data) could cause
the model to mistakenly attribute the effect of one hazard to the other.
Further, it may become impossible in this case to estimate synergistic effects
between the two, as an indicator of an interaction between the two exposures
would be almost collinear with the two main effects.

In our analysis, we find that this is usually not the case for the four tropical
cyclone hazards that we consider. Instead, we find that for most storms, the
hazards are fairly well-separated in terms of the counties they effect. There is
a particularly strong pattern where inland counties can be exposed to extreme
rain and flooding, but not wind. This separation opens the door for some
interesting study designs and statistical modeling. For example, these patterns
suggest that a study could investigate whether storm-associated rains alone
create a health risk, as there are many exposures with only rain in inland
counties, and so health associations could be explored here and contrasted with
coastal areas that experienced both wind and extreme rain.


\begin{shaded}
\textbf{R2 Comment 3:}
Line 394: It looks like the correlation coefficients can be very low if measured
from 75mm or somewhere around.  This seems to be a significant issue for
analyzing storms with heavier rainfall using NLDAS-2 data.

\textit{Line 392--394 from original manuscript:}

\begin{quotation}
\noindent
\textit{"Within these counties, storm-related rainfall measurements were
well-correlated between the two data sources, with rank correlations (bottom
right of each graph in Figure 2) between 0.87 and 0.98. "}
\end{quotation}
\end{shaded}

**Response:** 

This is a great idea. In the original manuscript, we describe this lower
correlation at higher precipitation values through the use of examples from
specific storms, and discuss the potential reasons for it, but measuring the
correlation specifically within these higher-precipitation events would add more
quantitative evidence to this discussion.

Here is the text where we discuss this point in the original manuscript:

> **Text in original manuscript:**
"There was some evidence that our primary rainfall metric may tend to
underestimate rainfall totals in storms with extremely high rainfall, based on
a few heavy-rainfall storms in Harris County, TX, Mobile County, AL, Charleston
County, SC, and Wake County, NC (Figure 2)."

> **Text in original manuscript:**
"The rainfall data are generally well-correlated with ground-based observations,
but may sometimes underestimate very high rainfall values (Figure 2). When
rainfall data is used to create binary exposure classifications, this
disagreement is unlikely to influence results, as both data sources agree in
identifying these as storms with high rainfall, but would be important to
consider for cases that include rainfall as a continuous measurement."

We have added estimates of these correlations specifically for
high-precipitation events in a new table in the Supplemental Material (Table S1,
reproduced as Table \@ref(tab:highprecipcorr) of this response document).

\input{tables/precip_high_corr.tex}

\begin{shaded}
\textbf{R2 Comment 4:}
Figure 4: I would add Y-axis label "NOAA classification."
\end{shaded}

**Response:** 

This is a great suggestion. We have made this change, as well as several changes
in response to EHP's Request 4 later in this response.

The original and revised versions of Figure 4 of the paper are reproduced in
Figure \@ref(fig:fig4) of this response.

```{r fig4, fig.cap="Reproduction of Figure 4 from the original manuscript (top) and revised version in the resubmitted manuscript (bottom). In the revised version, we have added a title to the y-axis ('Classification based on NOAA Storm Events Data'), binned the values shown by each point's fill, so these colors are easier to distinguish, increased the size of the points, and changed the algorithm for jittering, so none of the points overlap.", fig.show = "hold", fig.align = "center", out.width="100%"}
include_graphics("../figures/floodcomparison.pdf")
include_graphics("figures/floodcomparison.pdf")
```

\begin{shaded}
\textbf{R2 Comment 5:}
Table 2: This is another confusing table. Not sure what compared here. The
second column is a mean \# of exposed counties over years and the third column
seems like showing single storm with the max \# of exposed counties. Not sure
what info I should digest from this table.
\end{shaded}

> **Text from Results of original manuscript:**
"Across the four storm hazards considered, there was wide variation in the
average number of county exposures per year (Table 2). For tropical cyclone
tornadoes, there were on average about 40 county exposures per year within our
study. County exposures were more frequent for tropical cyclone wind exposures
(>160/year on average), even more frequent for tropical cyclone flood exposures
(>190/year on average), and most frequent for tropical cyclone rain exposure
(>290/year on average). For every hazard except tornadoes, we identified at
least one tropical cyclone that exposed over 250 counties (Table 2). However,
the largest-extent tropical cyclone varied across hazards: Frances in 2004
exposed the most counties based on rain, Michael in 2018 based on wind, and Ivan
in 2004 based on flooding and tornadoes (Table 2)."

**Response:** 

We hope that these results can help epidemiologists who are planning new
multi-year studies of tropical cyclones and associated health risks. For 
planning a multi-year study, it is helpful to know how many exposures the 
study might incorporate, as this can help in giving a general idea of whether
the study might have adequate statistical power to investigate a certain 
research question. This table provides some basic summaries of the extent
of exposure to different tropical cyclone hazards, both on average for the
storms in the study and also for those storms that were most extensive. 
Further, as with other results in the study, it provides evidence that 
patterns in different hazards can vary substantially, both because some 
hazards are much less likely to expose counties (e.g., torndaoes are a fairly 
rare exposure, based on this table) and also because there is variation in 
the most extensive storms across the hazards considered.

We have added text to help the readers interpret this table ...

\begin{shaded}
\textbf{R2 Comment 6:}
Subsection of Results on \textit{Agreement across exposure metrics}: 
As stated below, storms bring different hazards to different locations with
different time. What would be new learning by comparing these different pairs of
exposure metrics. Somewhat apple and orange comparison. Maybe providing more
explanation can help what the important learning from this is. I may not catch
it. Similar comment was added earlier.
\end{shaded}

**Response:** 

\begin{shaded}
\textbf{R2 Comment 7:}
Lines 494--495: Why this agreement is important? Same questions added above
already, but still wonder what I missed. It seems like the authors try to
develop an exposure matrix that is common in occupational epidemiology, but it
is only valid method when the exposures are similar and lead to the same health
outcomes.  Rain, wind, flood, and tornado should be treated separately, maybe
except rain and flood. It is possible to be an extreme storm with only severe
wind without major impacts from other exposures. Tornado is a good example.

\textit{Line 491--495 from original manuscript:}

\begin{quotation}
\noindent
\textit{"For another set of tropical cyclones (e.g., Ernesto in 2006, and Bertha
in 1996, Isabel in 2003), there was moderate to good agreement for pairwise
combinations of distance, rain, and wind, but poor agreement for other
combinations of metrics, while for another set of storms (e.g., Matthew in 2004
and Katrina in 2005), there was moderate to good agreement between distance and
rain."}
\end{quotation}
\end{shaded}

**Response:** 

> "A job exposure matrix (JEM) may be defined as a cross classification of jobs
and occupational exposures. Some matrices are based on tasks, instead of jobs. 
... Usually there is a group of experts who build the matrix based on their 
own knowledge and experience on occupational exopsures. The experts should 
assess exposure to an open or fixed list of occupational exposures (that can 
include chemical, physical, biological, and/or psychosocial agents) for an
open or fixed list of jobs (or tasks) usually coded according to some established
national or international classification system. Some JEMs also include 
information for different calendar time periods." [@garcia2003glossary]

[New advances in multi-pollutant analysis.]

[Some of the hazards could create similar risks, if they converge on a common
pathway of risk. For example, if pathway is through stress from property damage,
individually and in the county, then several hazards could indeed lead to the
same health outcomes (e.g., PTSD, other psychological markers, risk of acute
cardiovascular outcomes, etc.). One example is extreme level of destruction both
from wind (Hurricane Andrew, Hurricane Hugo), from river flooding associated
with among of precipitation (Hurricane Floyd, Hurricane Florence), and from
flash flooding associated with the rate of precipitation (Virginia storms).]

[A better future direction might be through regression models with multiple
explanatory pathways. This allows the model to fit different weights to each.
Other models can also be explored, like regression trees, which might have an
easier time capturing non-linearity (if exposure is included as a continuous
value, as with wind and rain) and interactions when there are co-exposures to
two or more hazards. Both of these are able (and will) either exclude or give a
very low weight to an explanatory variable that does not help much in explaining
variation in the health outcome.]

\begin{shaded}
\textbf{R2 Comment 8:}
Line 515: It would be informative to provide correlation coefficient for only
above 75 mm measures.

\textit{Line 514--515 from original manuscript:}

\begin{quotation}
\noindent
\textit{"The rainfall data are generally well-correlated with ground-based
observations, but may sometimes underestimate very high rainfall values (Figure
2)."}
\end{quotation}
\end{shaded}

**Response:** 
This is a great suggestion and helps provided quantitative support to the text
in the paper that the reviewer highlighted when making this comment. We have
added a table in the Supplemental Material that provides the Spearman
correlation coefficients for both all tropical cyclones to come near the sample
counties (replicating the analysis shown in Figure 2 of the main text) and
then when the analysis is limited to only those events with cumulative
precipitation of 75 mm or higher based on either data source.

In several cases, the correlation was much weaker when limiting analysis to
these severe rainfall events. For example, in Miami-Dade County, FL, the
Spearman correlation coefficient was 0.94 for all 64 tropical cyclones that came
near the county over the analysis period, while it was only 0.49 among the 18
storms with cumulative precipitation of 75 mm or more. There was a similar
reduction in Mobile County, AL. However, for some counties, the correlation
remained high (e.g., Harris County, TX; Orleans Parish, LA), although in all
cases there are small numbers of extreme precipitation events (5--20 across
these sample counties), and so there is likely substantial random variation in
these correlation estimates, since they are based on small numbers of 
observations.

[Include copy of table here.]

Finally, we have added a bit to the text that the reviewer cited to mention 
this point and to direct readers to the added supplemental table: 

> Text in revised manuscript (relevant addition in bold):
"The rainfall data are generally well-correlated with ground-based
observations, but may sometimes underestimate very high rainfall values (Figure
2), **and in some counties the correlation was substantially lower when 
considering only tropical cyclones with cumulative local rainfall of 75 mm
or more (Supplemental Table 1)**."

\begin{shaded}
\textbf{R2 Comment 9:}
Line 602: This is not a new finding, but how much misclassified would be a good
question to answer.

\textit{Line 600--602 from original manuscript:}

\begin{quotation}
\noindent
\textit{"Based on our results, the use of a distance-based metric to assess
exposure to any of these hazards, or the use of measurements from one hazard as
a proxy for exposure to any of the other hazards considered, would often
introduce exposure misclassification."}
\end{quotation}
\end{shaded}

**Response:** 
[Add new analysis of what percent of counties misclassified if distance 
is used as proxy for each hazard?]

\begin{shaded}
\textbf{R2 Comment 10:}
Lines 626--627: It would still be beneficial to treat each exposure as a single
variable first and then consider overall impact. Only if each exposure data are
available.

\textit{Line 626--628 from original manuscript:}

\begin{quotation}
\noindent
\textit{"For these storms, it may be possible to assess exposure to multiple
hazards of the storm using a single metric, perhaps even a proxy like the
distance between the county and the storm’s track."}
\end{quotation}
\end{shaded}

**Response:** 
We agree. This type of approach has not always been easy in the past, but can
be easily implemented with the dataset we publish here. We have added to the
text on this point: 

> Text in revised manuscript (relevant addition in bold):
"For these storms, it may be possible to assess exposure to multiple
hazards of the storm using a single metric, perhaps even a proxy like the
distance between the county and the storm’s track. **With the dataset we 
describe in this paper, however, there is little need to limit analysis
based on exposure to a single hazard or proxy, although multi-hazard studies
of storms with high agreement among hazard exposures should look out for 
modeling issues from multicollinearity.**"


# Reviewer 3: {-#reviewer-3}

\begin{shaded}
\textbf{Reviewer 3 overview:}
This paper describes the development of a dataset and R package that can be used
to assess exposure to tropical cyclone-related hazards (e.g., wind,
precipitation, flooding) at the county level in the eastern United States, as
well as a descriptive analysis of county-level exposure to these hazards. This R
package is novel and I believe of high interest to researchers interested in the
health effects of tropical cyclones.  I have a few minor suggestions to further
improve the clarity of the manuscript.
\end{shaded}

**Response:** 

\begin{shaded}
\textbf{Reviewer 3 (R3) Comment 1:}
Starting on line 128, the authors write "Extreme winds are more common to the
track's right, where counterclockwise cyclonic winds move in concert with the
tropical cyclone's forward motion". I think this statement may be specific to
tropical cyclones in the Northern hemisphere (i.e., those relevant to exposure
in the US). Would be good to clarify this.
\end{shaded}

**Response:** 

You are right, the opposite would be true in the Southern hemisphere. This
difference is driven by the Coriolis effect on cyclonic rotation, in which the
rotation of the earth diverts winds as they move toward the low pressure center
of the cyclone. We have edited the text to specify that this statement is
specific to the Northern Hemisphere:

> **Text in revised manuscript (relevant addition in bold):**
"**In the Northern Hemisphere, cyclonic winds are counterclockwise,** so
extreme winds are more common to the track's right where cyclonic winds move in
concert with the tropical cyclone's forward motion."

\begin{shaded}
\textbf{R3 Comment 2:}
Line 186: What is meant by the phrase "synoptic times"?

\textit{Lines 185--187 from original manuscript:}

\begin{quotation}
\noindent
\textit{"We used tracking data from HURDAT2, which records the storm center’s
position at the synoptic times of 6:00 am, 12:00 pm, 6:00 pm, and 12:00 am
Coordinated Universal Time (UTC)."}
\end{quotation}
\end{shaded}

**Response:** 

Some weather data are collected at standardized times, so that observations from
different locations can be meaningfully integrated to provide a view of
large-scale (synoptic) weather systems and patterns [@usnwstime;
@willoughby2007hurricane; @willis2006cleveland; @raines1996getting]. This
practice extends back over 100 years, when weather data for the US were
collected by local weather offices and sent by telegram at regular times each
day into a central office, which used this information to create synoptic
weather maps that captured major weather systems in the county and from this
sent back forecasts to local weather offices, also at standardized times
[@willis2006cleveland; @raines1996getting]. Synoptic weather times are typically
set based on the Universal Time Constant, with times recorded in Coordinated
Universal Time (i.e., Greenwich Mean Time), and time stamps on weather maps and
data based on this time zone often is given a "Z" (for "Zulu", an earlier name
for this timezone) after the time [@usnwstime;
@morris2008time].

We have added text to clarify the meaning of "synoptic times" in the text:

> **Text in revised manuscript (relevant change in bold):**
"We used tracking data from HURDAT2, which records the storm center’s position
at **four standardized times for weather data collection (synoptic times),**
6:00 am, 12:00 pm, 6:00 pm, and 12:00 am Coordinated Universal Time (UTC)."

\begin{shaded}
\textbf{R3 Comment 3:}
Several of the exposures are calculated for the population mean center of each
county. From the reference list, it looks like the authors consistently used
population centers from 2010 throughout the years included in the dataset/R
package. If so, I suggest clarifying this in the text. 
\end{shaded}

**Response:** 

This is correct---the population centers from the US 2010 Decennial Census were 
used throughout. We have added text to the manuscript to clarify this: 

> **Text in revised manuscript (relevant addition in bold):**
"At each 15- interval, we measured the distance between the storm's
center and each county's population mean center, **as of the 2010 US Decennial
Census** (US Census Bureau 2020)."

\begin{shaded}
\textbf{R3 Comment 4:}
In the methods section starting on line 276, the authors describe the flooding
and tornado data from the NOAA storm events database. I'd suggest providing some
information about where the events in this database come from (e.g., that they
are reported events) here in addition to covering it in the discussion.

\textit{Lines 276--281 from original manuscript:}

\begin{quotation}
\noindent
\textit{"To identify flood- and tornado-based tropical cyclone exposures in US
counties, we matched storm tracks with event listings from the National Oceanic
and Atmospheric Administration (NOAA)’s Storm Event Database (NOAA NCEI 2020).
While this database has recorded storm data, particularly tornadoes, since 1950,
its coverage changed substantially in 1996 to cover more types of storm events,
including flood events (NOAA NCEI 2020). We therefore only considered flood
metrics of tropical cyclone exposure for storms in 1996 and later."}
\end{quotation}
\end{shaded}

**Response:** 

As suggested, we have added that these events are based on reports in the
Methods, in addition to our previous discussion of the point in the Discussion:

> **Revised text (relevant addition in bold):**
To identify flood- and tornado-based tropical cyclone exposures in US counties,
we matched storm tracks with event listings from the National Oceanic and
Atmospheric Administration (NOAA)’s Storm Event Database (NOAA NCEI 2020).
**Events are listed in this database based on reports from National Weather
Service personnel and other sources.** While this database has recorded storm
data, particularly tornadoes, since 1950, its coverage changed substantially in
1996 to cover more types of storm events, including flood events (NOAA NCEI
2020). We therefore only considered flood metrics of tropical cyclone exposure
for storms in 1996 and later."

\begin{shaded}
\textbf{R3 Comment 5:}
Line 294: What is meant by the phrase "traditional tornado event database"?

\textit{Lines 294--295 from original manuscript:}

\begin{quotation}
\noindent
\textit{"The tornado observations from this dataset form a traditional tornado
event database for the US, and so we did not further validate the tornado event
data."}
\end{quotation}
\end{shaded}

**Response:** 

This database is NOAA's database of records on tornado events in the United
States. It is the original source for the only other national tornado database
available for the country (the Storm Prediction Center [SPC]'s National Tornado
Database), which provides a version of the same data in which tornado events are
joined into a single record in cases where they cross county lines. There is no
other comparable source of data on tornado occurrences that could be used for a
comparison as validation.

The NOAA Storm Events database originated as a database for recording tornadoes
in the US in 1953, and was originally called the Climatological Data National
Summary [@Ashley2007]. The database has gone through a number of expansions to
cover more events, and it now includes reports of other types of
hydrometeorological disaster events, including floods, extreme heat and cold,
wildfires, frost, and hail. However, it has maintained its status as the
database of record for tornadoes in the United States.

We have changed the text in the manuscript to make this clearer: 

> "The tornado observations from this dataset---**along with the derived version
of the data available through the Storm Prediction Center's National Tornado
Database---are the tornado event data of record** for the US. **There are no other
collections of tornado data comparable in temporal or geographical scope,** and so
we did not further validate the tornado event data."



Examples of research papers that leverage this database as a record of tornado
events, or the version of it shared by the [Severe Storm ...] through their
website [ref], include ... .

[Is this database used to validate tornado forecasts / warnings?]

This database has evolved through several stages but has, since its earliest
iteration in the 1950s, served as the National Weather Service's [?] data of
record on tornado events. While there is also a database maintained by the 
[Severe Storm ...], the two databases have different names but almost 
identical information once data are aggregated to a daily, county-level 
measure of whether there was a tornado recorded in that county on that date. 
[Difference between the two and why there are two.]

There is no other tornado base at the same temporal and spatial scale that
we could use for validation.

This database serves as the basis for much of the large-scale (e.g., assessing
patterns over the US over long time periods) tornado-based research for the US. 
It does have some well-documented limitations. One is a notable increase over 
time in the number of tornadoes reported, particularly for the weakest, F0 [?]
tornadoes. [This stabilizes some in the age of radar?]

> "Weather events listed in the publication Storm Data are considered to be
the official database for tornadoes by the National Weather Service." 
[written by someone at the NWS Storm Prediction Center] [@mccarthy2003nws]

The US National Weather Service's Storm Prediction Center has a tornado database
called the "National Tornado Database" [@center2020storm]. This uses the NOAA
Storm Events database as its primary source [@center2020storm]. They note on their
website: 

> "The tables below provide the links to comma separated values (.csv) files for
tornadoes, hail, and damaging winds, as compiled from *NWS Storm Data*. Tornado 
reports exist back to 1950 while hail and damaging wind events date from 1955.

The NOAA Storm Events database, or the Storm Prediction Center's version of the
tornado database have been used in a number of studies, including in a number of
studies of tornado climatology [@tippett2016more; @brooks2003climatological;
@strader2015climatology].

> "The dataset we will use is the so-called smooth log of severe-weather reports
collected by the SPC and archived in the National Oceanic and Atmospheric 
Administration publication *Storm Data*." [@brooks2003climatological]

> "Data such as tornado magnitude, path length, path width, and so on are
recorded by the National Climatic Data Center (NCDC) as a public service and for
use in meteorological, climatological, and engineering studies (Edwards et al.
2013)." [@strader2015climatology]

Sometimes used in conjunction with other NWS data products: 

> "Initially, tornado cases available in the NWS's Damage Assessment Toolkit
(NOAA, 2014) were employed, which is a GIS-based framework for collecting,
storing, and retrieving damage survey data (Camp et al., 2014). This toolkit
provides a variety of tornado event filtering (tornado survey point, track,
footprint, and/or swath) and download options [key mark-up language (.kml) or
shapefile (.shp)], supplying an initial sample of georeferenced damage
assessments. In this study, a tornado footprint is defined as the maximum areal
extent of tornado intensity inferred by the damage, wind speed measured directly
by mobile Doppler radar, or assessed theoretically as the recorded length
multiplied with the maximum width as reported in Storm Data."
[@strader2015climatology]

> "To serve the public interest and the National Climatic Data Center (NCDC)
storm data record, the National Weather Service (NWS) documents the path length,
width, and maximum damage rating for every tornado county segment (NOAA
2007).[Footnote in manuscript: County segments of tornado paths are then
combined at the NWS Storm Prediction Center to yield a unified one-tornado
(ONETOR) dataset of whole-tornado records (Schaefer and Edwards 1999).] Such
data are used in meteorological and climatological research, as well as in
determining construction standard for critical infrastructure such as
high-tension electric lines and nuclear power plants (e.g., Ramsdell et al.
2007)." [@edwards2013tornado]

Sometimes used in conjunction with other datasets: 

> "The contiguous U.S. tornado fatality dataset utilized in this study was 
transcribed and compiled from two primary sources: (1) a long-term study of 
U.S. tornados by Grazulis (1993, 1997, hereafter Grazulis dataset) and 
(2) the National Climatic Data Center's Storm Data (NCDC 1959--2005) and 
'Storm Events' datavase. The Grazulis dataset included 'significant' tornado
events, including all events that are known to have produced a fatality, 
from 1680 to 1995. Storm Data reports from 1959 to 2005 were utilized to 
supplement the existing record of killer tornado events documented by 
Frazulis." [in a study of tornado fatalities] [@Ashley2007]

History: 

> "During the early 1950s, the US Weather Bureau began a concerted effort to
count all US tornadoes. In 1950, a list of tornadoes was provided in the
*Climatological Data National Summary*, but it was not until 1953, the first
full year of the issuance of Weather Bureau tornado watches, that the agency
began to formally count all tornadoes. The *Climaticalogical Data National
Summary* document evolved into *Storm Data* by 1959, which, in addition to
tornado data, included information on other storm perils, such as severe hail,
high winds, and floods (Grazulis 1993). Over the years, the tornado dataset has
been formatted and adjusted by the National Severe Storms Forecast Center and
Storm Prediction Center (SPC), and currently the National Tornado Database is
administered by the NWS Headquarters, SPC, and NCDC (McCarthy 2003)."
[@Ashley2007]

\begin{shaded}
\textbf{R3 Comment 6:}
Table 1: I suggest providing the rationale for picking the thresholds for
classifying continuous exposures into binary ones within the table. 
\end{shaded}

**Response:** 

For all outcomes, there are various reasonable choices that can be picked for
the threshold in defining exposure. There is not room to go into these details
within Table 1, so instead we added a separate table to the Supplemental
Material that provides both some reasoning behind the thresholds selected in
this study and also other reasonable thresholds that could be considered.
Ultimately, this choice will depend on what a researcher considers as plausible
causal pathways for the outcome he or she is studying. For example, if power
outages form a plausible step in the pathway, then the threshold for gale-force
or strong gale-force winds might be reasonable, while if the pathway is more
likely to be through larger-scale building destruction, a higher threshold would
be more reasonable. Some studies have done sensitivity analyses to consider how
results changed using different exposure metrics. For example, @Czajkowski2011
performed a sensitivity analysis using different assumptions about wind decay
away from the center of a storm, while @Currie2013, who used a distance-based
exposure metric, considered different buffer distances as a sensitivity
analysis.


- **Wind:** This threshold is the threshold for gale-force and higher winds
based on the Beaufort wind scale. This value is used by the National Hurricane
Center [?] as the outer limit of winds for its wind radii product in storm 
tracking data. Other reasonable choices for a threshold of wind exposure 
could include the two more intense thresholds also used in the NHC [?] wind
radii estimates of storm extent (50 knots and 64 knots, the later of which 
is the threshold for hurricane-force winds on the Beaufort Scale) or other
thresholds on the Beaufort Scale (e.g., "strong gale", which starts at 
41 knots; "storm", which starts at 48 knots; "violent storm", which starts
at 56 knots).

- **Rain:** This threshold is ... Other reasonable thresholds might be ... or
might incorporate different ranges of days around the storm's closest approach
to a county in calculating the cumulative precipitation. For example, ... For
the most part, using our window (two days before to one day after a storm's
closest approach) captured most of the storm-associated precipitation that would
have been counted using a wider window from five days before to two days after a
storm's closest approach, even for a large, slow storm. However, for a storm
that loops and so passes some counties twice, such as Allison (2001), this
four-day window may not be adequate, and lead to an underestimation of total
rainfall associated with the storm relative to estimates for faster-moving
storms. Other storms that looped over or near land and so passed close to US
counties twice include Allison (1989), Alberto (1994), Gordon (1994), Dennis
(1999), Edouard (2002), Fay (2002), Alex (2004), Ivan (2004), Dennis (2005), and
Ophelia (2005). One difficulty in measuring rain exposure consistently across
many different storms is how many days to include in the rain total. Some
storms, especially ones associated with dangerous rainfalls, can move very
slowly, meaning a window of several days needs to be used to fully capture
storm-related rainfall. For example, one study used a period of over four days
to provide storm rainfall totals from the slow-moving Hurricane Danny in 1997
[@Medlin2007]. Single-storm studies often measure rainfall as the total rainfall
over the full "event" (e.g., Medlin2007); however, the timing of the event is
determine in a customized way for each storm, making it difficult to extend to
multi-storm analyses. We included more days prior to the storm because
storm-related rains often precede the passing of the storm's center [ref]. 


- **Distance:** This threshold is one that has been commonly used in studies in
which distance from the storm's track is used as a proxy for tropical cyclone
exposure (for example, [refs]). Further, [anything in relation to storm size?].
Other reasonable thresholds for distance include ..., all of which have also
been used in previous research in assessing tropical cyclone exposure using
distance from the storm's track as a proxy. Other distance-based exposure
metrics have defined "exposure" to occur only when a storm's center track passes
through a county's boundary (e.g., @Zandbergen2009, who describe this metric of
exposure as a "hit" on the county; @Kinney2008 is another example that
considered highest exposures for counties directly on the storm's path). This
way of measuring exposure can exclude nearby counties that suffered extreme
conditions from the storm but were not directly on the hurricane's track
[@Kruk2010], especially since hurricanes can have a width ([typical storm width;
citation])[JMF-- Weatherford and Gray paper referenced in Grabich 2016 may be a
good place to start. Suggest 30-60km width of the eye of the storm itself] that
is much larger than a typical county's width and since a hurricane's maximum
winds can extend well away from the storm's center. Further, exposure assessed
using this method is associated with the size [@Zandbergen2009; @Kruk2010] and
shape [@Zandbergen2009] of a county. (While some studies have avoided this
problem by determining exposure for equal-area grid cells (e.g., [@Kruk2010]),
in studies where exposure needs to be matched with a county-level outcome, some
process of aggregation from grid cells to counties would still need to be
performed.) By contrast, assessing distance-based exposure based on measuring
distance between the storm track and each county's center would be less
susceptible to differences in county size and shape and so might be a more
reliable metric of exposure than determining if a storm track ever crossed a
county boundary. Further, assessing the minimum distance between county center
and the storm track, as we do here, allows for a continuous, rather than binary,
metric of distance-based exposure to a storm. To date, distance parameters
involved with assessing risk of a particular storm have been rather arbitrary,
contributing to the necessity in understanding how such parameters influence a
county's exposure status. In assessing the risk of a given storm based on
distance, recent studies have defined the distance from the storm track affected
by a given storm differently. @Czajkowski2011 assessed county-level risk and
exposure based on a three-tierd definition, with primary counties being those
closest to the storm track on either side, secondary counties being adjacent to
primary coutnies, and tertiary counties adjacent to secondary counties. Such a
definition resulted in an exposure defenition based on an average distance
radius of 120 km [75 miles] on either side of the storm track [@Czajkowski2011].
Such a distance is slightly larger than that commonly used by public health
departments (i.e., 100 km) [@Czajkowski2011]. Other studies used distance
buffers of 30 kilometers [@Currie2013] or of 30, 60, and 100 kilometers as
thresholds for identifying distance-based exposure [@Grabich2016] [check the
Grabich paper-- was this distance to any part of a county, or to the center of
the county?; JMF-- I think any part of the county]. Another study used a
distance criterion of 60 kilometers from the storm track, classifying all
counties for which the 60 km buffer fully or partially passed through the
county's boundaries as exposed [@Grabich2015] [again, check to see if this was
to any point in the county or to some center point of the county][JMF-- it is
not abundantly clear in the Grabich 2016 article; based on their graphs it seems
like they considered a county exposed if the definition they used crossed any
county boundary. The Grabich 2015 paper specifies both full and partial counties
being included]. In a study assessing the association between hurricanes and
undesireable birth outcomes, researchers found that results were not sensitive
to the omission of residences 100 km from the storm path, and that results
varied insignificantly from 30-75 km [@Currie2013]. One study used distance to
determine exposure to landfalling US hurricanes by assigning counties along the
storm's tracks as "primary" exposures, counties neighboring primary counties as
"secondary" counties, and counties neighboring secondary counties as "tertiary"
counties; this method correspond to a distance buffer of approximately 75 miles
(120 kilometers) from the storm track for a county to qualify as "exposed" to a
storm [@Czajkowski2011]. The study found that about 30 direct hurricane
fatalities over their study period (1970--2007) occurred outside of this
distance range from the storm center [@Czajkowski2011], suggesting that this
distance choice can cause some counties with dangerous exposures to a storm to
be mis-identified as "unexposed". Tropical cyclones vary dramatically in size:
US tropical cyclones have been observed with radii to maximum winds as small as
20-km and as large as 200-km [@mallin2006; @quiring2011variations].




\begin{shaded}
\textbf{R3 Comment 7:}
Figure 3: The x-axis refers to one of the two wind data sources as "Extended
Best Tracks." Is this the same thing as the wind radii dataset described in the
text? Is so, I suggest clarifying this in the figure label.
\end{shaded}

**Response:** 

Yes, it is the same. We have revised the x-axis on the figure to clarify that
the comparison is based on the wind radii dataset described in the text, as
requested. The original and revised versions of this paper are reproduced in
this response document as Figure \@ref(fig:fig3).

```{r fig3, fig.cap="Reproduction of Figure 3 from the original manuscript (left) and revised version in the resubmitted manuscript (right). In the revised version, the x-axis has been labelled to clarify that this axis is showing results based on the wind radii dataset described in the text, in response to Reviewer 3's Comment 7. The revised version also uses a more focused range for the x axis, as requested in EHP's Request 1. Figure caption in the revised manuscript: \\textit{Comparison of two sources of wind exposure estimates: (1) modeled peak sustained surface wind and (2) estimates based on HURDAT2's wind speed radii. Each point represents a storm, with the x-axis giving the percent of counties classified in the same category of peak sustained surface wind (< 34 kt; 34--49.9 kt; 50--63.9 kt; $\\ge$64 kt) by both sources of data. The color of each point gives the number of study counties that were exposed to peak sustained surface wind of at least 34 kt (based on modeled wind). Estimates are shown for study storms since 2004, the earliest year for which post-storm reanalysis wind speed radii are routinely available in HURDAT2, and for which at least one study county had a peak sustained wind of $\\ge$34 kt based on the post-storm wind radii.}", fig.show = "hold", out.width="50%"}
include_graphics("../figures/windcomparison.pdf")
include_graphics("figures/windcomparison.pdf")
```

# Requests from EHP: {-#requests-from-ehp}

\begin{shaded}
\textbf{EHP Request 1:}
Figure 3: Please truncate x-axis scale as you do for Figure S7. Since smallest
value is $\sim75\%$, this would make the data in the relevant range much easier to
appreciate.
\end{shaded}

**Response:** 

[Typo here? I don't think there's a Figure S7.]

We have revised this figure to show an x-axis truncated at the minimum observed
value ($\sim75\%$), as requested. The original and revised versions of this
paper are reproduced in this response document as Figure \@ref(fig:fig3).

\begin{shaded}
\textbf{EHP Request 2:}
Main text figures (general)
To ensure that figures are accessible to readers with impaired color vision, do not use color as the sole means of conveying information in a figure unless absolutely necessary. Use different symbols, shading/textures, and line patterns instead of (or in addition to) color to distinguish among different data points. Use contrasting colors that can be easily distinguished from each other when the figure is printed in black and white. For details, see: \url{https://ehp.niehs.nih.gov/authors/figures}.
\end{shaded}

**Response:** 

In all these figures, we have used colormaps that are both accessible to readers
with several forms of color vision deficiency, including the most common type
(deuteranomaly, a type of red-green color vision deficiency), and can be easily
distinguished when the figure is printed in black and white [@viridis;
@van2015mpl].

For these figures, we selected colormaps that were created to meet both these
criteria. These colormaps are all multi-hue sequential maps that avoid red-green
contrasts, while maximizing dynamic range in comparison to something
single-hued, like grayscale [@nunez2018optimizing; @viridis]. They are all
perceptually uniform across the scale for those with normal color vision
[@liu2018somewhere] and are close to perceptually uniform for those with common
forms of color deficiency [@nunez2018optimizing]. They reproduce their original
sequential gradient when printed in grayscale and so are perceptable when
printed on a black-and-white printer [@van2015mpl; @nunez2018optimizing].
Previous research has found that colormaps in this family allow viewers to more
quickly and accurately judge relative distances in scientific figures
[@liu2018somewhere]. The family of colormaps we used in the figures is currently
considered the gold standard for scientific figures [@nunez2018optimizing].

For example, in Figures \@ref(fig:fig5check1) and \@ref(fig:fig5check2) (left)
of this response document, we have reproduced the current version of Figure 5 as
it would appear to someone with three common types of color vision deficiencies:
deuteranomaly (defective green cone cells, the most common type), as well as
protanopia (defective red cone cells) and tritanopia (defective blue cone
cells). We have also shown how the Figure would appear in grayscale in
\@ref(fig:fig5check2) (right). We have included similar versions of other
figures from the manuscript in response to later requests from EHP in this
response.

```{r fig5check1, fig.cap="Reproduction of revised version of Figure 5 to check accessibility to readers with two types of color vision deficiency, deuteranopia (left) and protanopia (right).", out.width="90%", fig.align = "center"}
include_graphics("figures/averageexposureonly_check1.pdf")
```

```{r fig5check2, fig.cap="Reproduction of revised version of Figure 5 to check accessibility to readers with a third type of color vision deficiency, tritanopia (left) and for printing the figure in grayscale (right).", out.width="90%", fig.align = "center"}
include_graphics("figures/averageexposureonly_check2.pdf")
```


\begin{shaded}
\textbf{EHP Request 3:}
Figure 3: Please check the contrast between the colors used to indicate numbers
of counties. If you can see the gradient when printed in black and white it
should be ok, but some of the differences are very subtle. Truncating the x-axis
might help with this (if possible.)
\end{shaded}

**Response:** 

As we note in our response to EHP Request 2, we used colormaps for all
figures that are designed to be perceptible both for those with common types of
color vision deficiency and when printed in grayscale. We have increased the
size of the circles in Figure 3 of the revised manuscript (reproduced in Figure
\@ref(fig:fig3) of this response) so that the colors are clearer, as well as
truncated the x-axis as requested in EHP Request 1. Also, we show in Figures
\@ref(fig:fig3check1) and \@ref(fig:fig3check2) of this response how the revised
Figure 3 would look with three common types of color vision deficiency as well
as when printed in black and white. The gradient is clear when this figure is
printed in black and white.

```{r fig3check1, fig.cap="Reproduction of revised version of Figure 3 to check accessibility to readers with two types of color vision deficiency, deuteranopia (left) and protanopia (right).", out.width="100%"}
include_graphics("figures/windcomparison_check1.pdf")
```

```{r fig3check2, fig.cap="Reproduction of revised version of Figure 3 to check accessibility to readers with a third type of color vision deficiency, tritanopia (left) and for printing the figure in grayscale (right).", out.width="100%"}
include_graphics("figures/windcomparison_check2.pdf")
```

\begin{shaded}
\textbf{EHP Request 4:}
Figure 4: Even with nominally normal color vision and a large high-resolution
monitor, it is difficult for me to make out the color differences in the
symbols---almost all appear black or very dark blue, a dozen or so look yellow,
and a handful look dark grey. Is there any way to increase the contrast in these
colors? Would you consider binning into categories instead of using a
continuous color gradient?
\end{shaded}

**Response:** 

Thank you for these suggestions. 

For the original figure, most of the events resulted in 0% of flood gages
exceeding the flood threshold, especially for events that were not recorded as
floods based on NOAA Storm Events. Therefore, most of the points in the original
were indeed dark blue, representing 0%, but we can see that readers might have
questioned if this was the case or if the scale made it harder to see subtler
differences.

We have reproduced both the original and revised versions of Figure 4 as Figure
\@ref(fig:fig4) of this response. We have made several changes to make this
figure easier for readers to see and interpret. These changes include:

- Binning the values shown by the points fill, so that readers only need to 
be able to distinguish among four colors, rather than across a larger scale.
- Increasing the size of the points, so the fill in each point is easier to 
see. 
- Using a different algorithm to "jitter" the points, so there are no points that
overlap each other. 

\begin{shaded}
\textbf{EHP Request 5:}
Figures 5--6: Please check to see if the colors used can be distinguished if
printed in black and white, and if not, use a “color-blind friendly” palette if
possible. If this is not feasible, we can request and exemption from
accessibility requirements, but we try to do this only when alternatives  would
degrade the information content of  the figure.
\end{shaded}

**Response:** 

As we note in our response to EHP Request 2, we used colormaps for all figures
that are designed to be perceptible both for those with common types of color
vision deficiency and when printed in grayscale Figures \@ref(fig:fig5check1)
and \@ref(fig:fig5check1) show what Figure 5 would look like under three common
types of color vision deficiency and when printed in black and white. Figures
\@ref(fig:fig6check1) and \@ref(fig:fig6check2) show the same thing for Figure
6. Therefore, these figures should not require any exemption from accessibility
requirements, as they should satisfy these requirements.

```{r fig6check1, fig.cap="Reproduction of revised version of Figure 6 to check accessibility to readers with two types of color vision deficiency, deuteranopia (left) and protanopia (right).", fig.show = "hold", out.width="50%"}
include_graphics("figures/jaccard_heatmap_check1.pdf")

include_graphics("figures/jaccard_heatmap_check2.pdf")
```

```{r fig6check2, fig.cap="Reproduction of revised version of Figure 6 to check accessibility to readers with a third type of color vision deficiency, tritanopia (left) and for printing the figure in grayscale (right).", fig.show = "hold", out.width="50%"}
include_graphics("figures/jaccard_heatmap_check3.pdf")
include_graphics("figures/jaccard_heatmap_check4.pdf")
```

\begin{shaded}
\textbf{R2 Comment 6:}
Supplemental Material: Please provide your supplemental material file in Word
(.docx) format. For additional information, see:
\url{https://ehp.niehs.nih.gov/authors/supplemental-material}. \end{shaded}

**Response:** 

We have provided the Supplemental Material in the Word file format in our
revised submission.

# References for the response {-#references-for-the-response}
